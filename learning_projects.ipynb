{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4khxHXs7dS4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_diabetes()"
      ],
      "metadata": {
        "id": "cPzKvr-Y7zsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , y = data.data, data.target"
      ],
      "metadata": {
        "id": "OkJufQAz8gdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column = data.feature_names\n",
        "column = list(set(column))\n",
        "column.remove('Target')\n",
        "print(column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrMBFxAf7VOJ",
        "outputId": "ce0f7f23-18f7-48f5-d9a2-04d8fc7660b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s1', 's2', 'bp', 's3', 's6', 's5', 'age', 's4', 'sex', 'bmi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(X,columns=column)"
      ],
      "metadata": {
        "id": "0hkUWlVW6gJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB435FvC89YE",
        "outputId": "5f6a9837-e263-42d6-8ed0-0d1e29f56102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"n1\"] = df[\"bp\"]*df[\"bmi\"]"
      ],
      "metadata": {
        "id": "1wwayEVN_KDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "3h9kpHSVAdN1",
        "outputId": "8cb18f5f-bad3-4ee7-ab5e-0a45318914ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           s1        s2        bp        s3        s6        s5       age  \\\n",
              "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
              "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
              "2    0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
              "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
              "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
              "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
              "439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n",
              "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
              "441 -0.045472 -0.044642 -0.073030 -0.081413  0.083740  0.027809  0.173816   \n",
              "\n",
              "           s4       sex       bmi        n1  targets  \n",
              "0   -0.002592  0.019907 -0.017646 -0.001089    151.0  \n",
              "1   -0.039493 -0.068332 -0.092204  0.004746     75.0  \n",
              "2   -0.002592  0.002861 -0.025930 -0.001153    141.0  \n",
              "3    0.034309  0.022688 -0.009362  0.000109    206.0  \n",
              "4   -0.002592 -0.031988 -0.046641  0.001697    135.0  \n",
              "..        ...       ...       ...       ...      ...  \n",
              "437 -0.002592  0.031193  0.007207  0.000142    178.0  \n",
              "438  0.034309 -0.018114  0.044485 -0.000708    104.0  \n",
              "439 -0.011080 -0.046883  0.015491 -0.000246    132.0  \n",
              "440  0.026560  0.044529 -0.025930 -0.001013    220.0  \n",
              "441 -0.039493 -0.004222  0.003064 -0.000224     57.0  \n",
              "\n",
              "[442 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2994fa6-d9c7-4c6f-a466-b63584a39df3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>bp</th>\n",
              "      <th>s3</th>\n",
              "      <th>s6</th>\n",
              "      <th>s5</th>\n",
              "      <th>age</th>\n",
              "      <th>s4</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>n1</th>\n",
              "      <th>targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019907</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>-0.001089</td>\n",
              "      <td>151.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068332</td>\n",
              "      <td>-0.092204</td>\n",
              "      <td>0.004746</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005670</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002861</td>\n",
              "      <td>-0.025930</td>\n",
              "      <td>-0.001153</td>\n",
              "      <td>141.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022688</td>\n",
              "      <td>-0.009362</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>206.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031988</td>\n",
              "      <td>-0.046641</td>\n",
              "      <td>0.001697</td>\n",
              "      <td>135.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.019662</td>\n",
              "      <td>0.059744</td>\n",
              "      <td>-0.005697</td>\n",
              "      <td>-0.002566</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.031193</td>\n",
              "      <td>0.007207</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>178.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>-0.005515</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>-0.067642</td>\n",
              "      <td>0.049341</td>\n",
              "      <td>0.079165</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>-0.018114</td>\n",
              "      <td>0.044485</td>\n",
              "      <td>-0.000708</td>\n",
              "      <td>104.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>0.017293</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.013840</td>\n",
              "      <td>-0.024993</td>\n",
              "      <td>-0.011080</td>\n",
              "      <td>-0.046883</td>\n",
              "      <td>0.015491</td>\n",
              "      <td>-0.000246</td>\n",
              "      <td>132.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.016318</td>\n",
              "      <td>0.015283</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.026560</td>\n",
              "      <td>0.044529</td>\n",
              "      <td>-0.025930</td>\n",
              "      <td>-0.001013</td>\n",
              "      <td>220.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.073030</td>\n",
              "      <td>-0.081413</td>\n",
              "      <td>0.083740</td>\n",
              "      <td>0.027809</td>\n",
              "      <td>0.173816</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.004222</td>\n",
              "      <td>0.003064</td>\n",
              "      <td>-0.000224</td>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows Ã— 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2994fa6-d9c7-4c6f-a466-b63584a39df3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f2994fa6-d9c7-4c6f-a466-b63584a39df3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f2994fa6-d9c7-4c6f-a466-b63584a39df3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(\"targets\",axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "CbR7rCFH_0fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.astype(np.float32)"
      ],
      "metadata": {
        "id": "sIe4-M3y9LBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0X4oWirp9u2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(df,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "yUskZAOHABUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "6NEqdV4k-OTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1= X_train1.values\n",
        "X_train1 = torch.from_numpy(X_train1.astype(np.float32))\n",
        "X_test1 = X_test1.values\n",
        "X_test1 = torch.from_numpy(X_test1.astype(np.float32))"
      ],
      "metadata": {
        "id": "RnAinkjNCG8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1 = torch.from_numpy(X_test1)"
      ],
      "metadata": {
        "id": "F4b3erD1A4WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train1 = torch.from_numpy(y_train1.astype(np.float32))\n",
        "y_test1 = torch.from_numpy(y_test1.astype(np.float32))"
      ],
      "metadata": {
        "id": "etaXSx_4A4Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mod(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lay1 = nn.Linear(X_train1.shape[1],32)\n",
        "    self.lay2 = nn.Linear(32,1)\n",
        "  def forward(self,x):\n",
        "    x = self.lay1(x)\n",
        "    x = self.lay2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "SqNsjzSCA4bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUEJURz4Foh5",
        "outputId": "cd32dd85-bfbb-459a-a965-fa7ab98b04e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = mod()\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer1 = torch.optim.Adam(params = model1.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "p7IwWJStA4e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs1 = 500\n",
        "for epoch in range(epochs1):\n",
        "  model1.train()\n",
        "  y_hat1 = model1(X_train1)\n",
        "  loss_11 = loss_function(y_hat1,y_train1)\n",
        "  optimizer1.zero_grad()\n",
        "  loss_11.backward()\n",
        "  optimizer1.step()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    y_pred1 = model1(X_test1)\n",
        "    test_loss = loss_function(y_pred1,y_test1)\n",
        "    mae = nn.L1Loss()(y_pred1,y_test1)\n",
        "    print(f'Epochs = {epoch}........ Training Loss = {loss_11}....... Test Loss = {test_loss}........ MAE = {mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXdEUm-5A4iR",
        "outputId": "719c915a-9bd3-4618-bf0c-3e13087445cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs = 0........ Training Loss = 29774.21875....... Test Loss = 26581.431640625........ MAE = 145.88790893554688\n",
            "Epochs = 1........ Training Loss = 29746.146484375....... Test Loss = 26555.474609375........ MAE = 145.7989044189453\n",
            "Epochs = 2........ Training Loss = 29718.24609375....... Test Loss = 26529.6640625........ MAE = 145.7103729248047\n",
            "Epochs = 3........ Training Loss = 29690.505859375....... Test Loss = 26503.951171875........ MAE = 145.6221160888672\n",
            "Epochs = 4........ Training Loss = 29662.875....... Test Loss = 26478.26171875........ MAE = 145.53387451171875\n",
            "Epochs = 5........ Training Loss = 29635.26171875....... Test Loss = 26452.51953125........ MAE = 145.44540405273438\n",
            "Epochs = 6........ Training Loss = 29607.5859375....... Test Loss = 26426.626953125........ MAE = 145.3563995361328\n",
            "Epochs = 7........ Training Loss = 29579.73828125....... Test Loss = 26400.46484375........ MAE = 145.26634216308594\n",
            "Epochs = 8........ Training Loss = 29551.58203125....... Test Loss = 26373.890625........ MAE = 145.1748504638672\n",
            "Epochs = 9........ Training Loss = 29522.955078125....... Test Loss = 26346.75390625........ MAE = 145.0813446044922\n",
            "Epochs = 10........ Training Loss = 29493.69921875....... Test Loss = 26318.91015625........ MAE = 144.9853515625\n",
            "Epochs = 11........ Training Loss = 29463.642578125....... Test Loss = 26290.2109375........ MAE = 144.88636779785156\n",
            "Epochs = 12........ Training Loss = 29432.634765625....... Test Loss = 26260.53125........ MAE = 144.7838592529297\n",
            "Epochs = 13........ Training Loss = 29400.521484375....... Test Loss = 26229.724609375........ MAE = 144.67742919921875\n",
            "Epochs = 14........ Training Loss = 29367.154296875....... Test Loss = 26197.67578125........ MAE = 144.56661987304688\n",
            "Epochs = 15........ Training Loss = 29332.392578125....... Test Loss = 26164.255859375........ MAE = 144.4509735107422\n",
            "Epochs = 16........ Training Loss = 29296.10546875....... Test Loss = 26129.3515625........ MAE = 144.33009338378906\n",
            "Epochs = 17........ Training Loss = 29258.162109375....... Test Loss = 26092.8515625........ MAE = 144.2035675048828\n",
            "Epochs = 18........ Training Loss = 29218.439453125....... Test Loss = 26054.654296875........ MAE = 144.071044921875\n",
            "Epochs = 19........ Training Loss = 29176.826171875....... Test Loss = 26014.6640625........ MAE = 143.93215942382812\n",
            "Epochs = 20........ Training Loss = 29133.2109375....... Test Loss = 25972.7890625........ MAE = 143.78659057617188\n",
            "Epochs = 21........ Training Loss = 29087.5....... Test Loss = 25928.947265625........ MAE = 143.63400268554688\n",
            "Epochs = 22........ Training Loss = 29039.595703125....... Test Loss = 25883.05078125........ MAE = 143.47409057617188\n",
            "Epochs = 23........ Training Loss = 28989.4140625....... Test Loss = 25835.037109375........ MAE = 143.30662536621094\n",
            "Epochs = 24........ Training Loss = 28936.87109375....... Test Loss = 25784.82421875........ MAE = 143.13125610351562\n",
            "Epochs = 25........ Training Loss = 28881.88671875....... Test Loss = 25732.34765625........ MAE = 142.94773864746094\n",
            "Epochs = 26........ Training Loss = 28824.384765625....... Test Loss = 25677.541015625........ MAE = 142.7558135986328\n",
            "Epochs = 27........ Training Loss = 28764.29296875....... Test Loss = 25620.341796875........ MAE = 142.55523681640625\n",
            "Epochs = 28........ Training Loss = 28701.54296875....... Test Loss = 25560.6796875........ MAE = 142.34568786621094\n",
            "Epochs = 29........ Training Loss = 28636.06640625....... Test Loss = 25498.50390625........ MAE = 142.12698364257812\n",
            "Epochs = 30........ Training Loss = 28567.791015625....... Test Loss = 25433.751953125........ MAE = 141.8988494873047\n",
            "Epochs = 31........ Training Loss = 28496.66015625....... Test Loss = 25366.36328125........ MAE = 141.66102600097656\n",
            "Epochs = 32........ Training Loss = 28422.603515625....... Test Loss = 25296.28125........ MAE = 141.41322326660156\n",
            "Epochs = 33........ Training Loss = 28345.55859375....... Test Loss = 25223.443359375........ MAE = 141.1552734375\n",
            "Epochs = 34........ Training Loss = 28265.46484375....... Test Loss = 25147.806640625........ MAE = 140.88681030273438\n",
            "Epochs = 35........ Training Loss = 28182.259765625....... Test Loss = 25069.306640625........ MAE = 140.607666015625\n",
            "Epochs = 36........ Training Loss = 28095.888671875....... Test Loss = 24987.90234375........ MAE = 140.31761169433594\n",
            "Epochs = 37........ Training Loss = 28006.291015625....... Test Loss = 24903.5390625........ MAE = 140.0162811279297\n",
            "Epochs = 38........ Training Loss = 27913.416015625....... Test Loss = 24816.16796875........ MAE = 139.7035369873047\n",
            "Epochs = 39........ Training Loss = 27817.212890625....... Test Loss = 24725.75390625........ MAE = 139.379150390625\n",
            "Epochs = 40........ Training Loss = 27717.630859375....... Test Loss = 24632.251953125........ MAE = 139.0428466796875\n",
            "Epochs = 41........ Training Loss = 27614.63671875....... Test Loss = 24535.62890625........ MAE = 138.69442749023438\n",
            "Epochs = 42........ Training Loss = 27508.18359375....... Test Loss = 24435.85546875........ MAE = 138.33367919921875\n",
            "Epochs = 43........ Training Loss = 27398.244140625....... Test Loss = 24332.90234375........ MAE = 137.96043395996094\n",
            "Epochs = 44........ Training Loss = 27284.78125....... Test Loss = 24226.748046875........ MAE = 137.574462890625\n",
            "Epochs = 45........ Training Loss = 27167.77734375....... Test Loss = 24117.365234375........ MAE = 137.1756134033203\n",
            "Epochs = 46........ Training Loss = 27047.208984375....... Test Loss = 24004.73828125........ MAE = 136.76368713378906\n",
            "Epochs = 47........ Training Loss = 26923.0546875....... Test Loss = 23888.859375........ MAE = 136.33848571777344\n",
            "Epochs = 48........ Training Loss = 26795.298828125....... Test Loss = 23769.701171875........ MAE = 135.8998565673828\n",
            "Epochs = 49........ Training Loss = 26663.9296875....... Test Loss = 23647.26953125........ MAE = 135.44761657714844\n",
            "Epochs = 50........ Training Loss = 26528.94140625....... Test Loss = 23521.54296875........ MAE = 134.9816436767578\n",
            "Epochs = 51........ Training Loss = 26390.326171875....... Test Loss = 23392.52734375........ MAE = 134.50173950195312\n",
            "Epochs = 52........ Training Loss = 26248.078125....... Test Loss = 23260.21875........ MAE = 134.0077362060547\n",
            "Epochs = 53........ Training Loss = 26102.20703125....... Test Loss = 23124.611328125........ MAE = 133.49952697753906\n",
            "Epochs = 54........ Training Loss = 25952.70703125....... Test Loss = 22985.716796875........ MAE = 132.9769287109375\n",
            "Epochs = 55........ Training Loss = 25799.59375....... Test Loss = 22843.541015625........ MAE = 132.4397735595703\n",
            "Epochs = 56........ Training Loss = 25642.8671875....... Test Loss = 22698.091796875........ MAE = 131.88800048828125\n",
            "Epochs = 57........ Training Loss = 25482.55078125....... Test Loss = 22549.3828125........ MAE = 131.3214111328125\n",
            "Epochs = 58........ Training Loss = 25318.658203125....... Test Loss = 22397.43359375........ MAE = 130.73988342285156\n",
            "Epochs = 59........ Training Loss = 25151.2109375....... Test Loss = 22242.259765625........ MAE = 130.14332580566406\n",
            "Epochs = 60........ Training Loss = 24980.234375....... Test Loss = 22083.896484375........ MAE = 129.53158569335938\n",
            "Epochs = 61........ Training Loss = 24805.76171875....... Test Loss = 21922.359375........ MAE = 128.90460205078125\n",
            "Epochs = 62........ Training Loss = 24627.82421875....... Test Loss = 21757.69140625........ MAE = 128.26226806640625\n",
            "Epochs = 63........ Training Loss = 24446.462890625....... Test Loss = 21589.923828125........ MAE = 127.60446166992188\n",
            "Epochs = 64........ Training Loss = 24261.72265625....... Test Loss = 21419.103515625........ MAE = 126.93114471435547\n",
            "Epochs = 65........ Training Loss = 24073.6484375....... Test Loss = 21245.275390625........ MAE = 126.24226379394531\n",
            "Epochs = 66........ Training Loss = 23882.294921875....... Test Loss = 21068.490234375........ MAE = 125.53775024414062\n",
            "Epochs = 67........ Training Loss = 23687.72265625....... Test Loss = 20888.80859375........ MAE = 124.81757354736328\n",
            "Epochs = 68........ Training Loss = 23490.005859375....... Test Loss = 20706.294921875........ MAE = 124.08173370361328\n",
            "Epochs = 69........ Training Loss = 23289.205078125....... Test Loss = 20521.017578125........ MAE = 123.33020782470703\n",
            "Epochs = 70........ Training Loss = 23085.408203125....... Test Loss = 20333.046875........ MAE = 122.5630111694336\n",
            "Epochs = 71........ Training Loss = 22878.689453125....... Test Loss = 20142.46484375........ MAE = 121.7801742553711\n",
            "Epochs = 72........ Training Loss = 22669.146484375....... Test Loss = 19949.353515625........ MAE = 120.98172760009766\n",
            "Epochs = 73........ Training Loss = 22456.86328125....... Test Loss = 19753.796875........ MAE = 120.16773223876953\n",
            "Epochs = 74........ Training Loss = 22241.947265625....... Test Loss = 19555.88671875........ MAE = 119.33824920654297\n",
            "Epochs = 75........ Training Loss = 22024.498046875....... Test Loss = 19355.712890625........ MAE = 118.49345397949219\n",
            "Epochs = 76........ Training Loss = 21804.615234375....... Test Loss = 19153.37890625........ MAE = 117.63349151611328\n",
            "Epochs = 77........ Training Loss = 21582.421875....... Test Loss = 18948.98046875........ MAE = 116.75885009765625\n",
            "Epochs = 78........ Training Loss = 21358.021484375....... Test Loss = 18742.6171875........ MAE = 115.86932373046875\n",
            "Epochs = 79........ Training Loss = 21131.537109375....... Test Loss = 18534.396484375........ MAE = 114.9655990600586\n",
            "Epochs = 80........ Training Loss = 20903.083984375....... Test Loss = 18324.427734375........ MAE = 114.0481948852539\n",
            "Epochs = 81........ Training Loss = 20672.787109375....... Test Loss = 18112.818359375........ MAE = 113.11766052246094\n",
            "Epochs = 82........ Training Loss = 20440.7734375....... Test Loss = 17899.67578125........ MAE = 112.17369079589844\n",
            "Epochs = 83........ Training Loss = 20207.16796875....... Test Loss = 17685.12109375........ MAE = 111.21742248535156\n",
            "Epochs = 84........ Training Loss = 19972.1015625....... Test Loss = 17469.26171875........ MAE = 110.24932861328125\n",
            "Epochs = 85........ Training Loss = 19735.708984375....... Test Loss = 17252.212890625........ MAE = 109.26995849609375\n",
            "Epochs = 86........ Training Loss = 19498.11328125....... Test Loss = 17034.099609375........ MAE = 108.27993774414062\n",
            "Epochs = 87........ Training Loss = 19259.462890625....... Test Loss = 16815.03515625........ MAE = 107.27984619140625\n",
            "Epochs = 88........ Training Loss = 19019.88671875....... Test Loss = 16595.142578125........ MAE = 106.27125549316406\n",
            "Epochs = 89........ Training Loss = 18779.525390625....... Test Loss = 16374.546875........ MAE = 105.2550277709961\n",
            "Epochs = 90........ Training Loss = 18538.513671875....... Test Loss = 16153.3662109375........ MAE = 104.23149108886719\n",
            "Epochs = 91........ Training Loss = 18296.99609375....... Test Loss = 15931.7294921875........ MAE = 103.20083618164062\n",
            "Epochs = 92........ Training Loss = 18055.107421875....... Test Loss = 15709.759765625........ MAE = 102.16427612304688\n",
            "Epochs = 93........ Training Loss = 17812.990234375....... Test Loss = 15487.5888671875........ MAE = 101.12290954589844\n",
            "Epochs = 94........ Training Loss = 17570.7890625....... Test Loss = 15265.341796875........ MAE = 100.07933044433594\n",
            "Epochs = 95........ Training Loss = 17328.642578125....... Test Loss = 15043.1484375........ MAE = 99.031494140625\n",
            "Epochs = 96........ Training Loss = 17086.6875....... Test Loss = 14821.138671875........ MAE = 97.9803237915039\n",
            "Epochs = 97........ Training Loss = 16845.0703125....... Test Loss = 14599.443359375........ MAE = 96.9290542602539\n",
            "Epochs = 98........ Training Loss = 16603.927734375....... Test Loss = 14378.1962890625........ MAE = 95.87687683105469\n",
            "Epochs = 99........ Training Loss = 16363.4052734375....... Test Loss = 14157.5263671875........ MAE = 94.82228088378906\n",
            "Epochs = 100........ Training Loss = 16123.63671875....... Test Loss = 13937.5673828125........ MAE = 93.77162170410156\n",
            "Epochs = 101........ Training Loss = 15884.7607421875....... Test Loss = 13718.451171875........ MAE = 92.72434997558594\n",
            "Epochs = 102........ Training Loss = 15646.9169921875....... Test Loss = 13500.30859375........ MAE = 91.67881774902344\n",
            "Epochs = 103........ Training Loss = 15410.240234375....... Test Loss = 13283.26953125........ MAE = 90.64016723632812\n",
            "Epochs = 104........ Training Loss = 15174.865234375....... Test Loss = 13067.4658203125........ MAE = 89.6090316772461\n",
            "Epochs = 105........ Training Loss = 14940.921875....... Test Loss = 12853.025390625........ MAE = 88.58573913574219\n",
            "Epochs = 106........ Training Loss = 14708.5458984375....... Test Loss = 12640.076171875........ MAE = 87.57188415527344\n",
            "Epochs = 107........ Training Loss = 14477.861328125....... Test Loss = 12428.74609375........ MAE = 86.5677261352539\n",
            "Epochs = 108........ Training Loss = 14248.998046875....... Test Loss = 12219.158203125........ MAE = 85.5757827758789\n",
            "Epochs = 109........ Training Loss = 14022.0791015625....... Test Loss = 12011.4365234375........ MAE = 84.60012817382812\n",
            "Epochs = 110........ Training Loss = 13797.228515625....... Test Loss = 11805.705078125........ MAE = 83.63884735107422\n",
            "Epochs = 111........ Training Loss = 13574.564453125....... Test Loss = 11602.080078125........ MAE = 82.69230651855469\n",
            "Epochs = 112........ Training Loss = 13354.201171875....... Test Loss = 11400.6796875........ MAE = 81.7634048461914\n",
            "Epochs = 113........ Training Loss = 13136.255859375....... Test Loss = 11201.6201171875........ MAE = 80.85375213623047\n",
            "Epochs = 114........ Training Loss = 12920.837890625....... Test Loss = 11005.01171875........ MAE = 79.96101379394531\n",
            "Epochs = 115........ Training Loss = 12708.05859375....... Test Loss = 10810.9619140625........ MAE = 79.08815002441406\n",
            "Epochs = 116........ Training Loss = 12498.015625....... Test Loss = 10619.5810546875........ MAE = 78.23382568359375\n",
            "Epochs = 117........ Training Loss = 12290.8173828125....... Test Loss = 10430.9677734375........ MAE = 77.39815521240234\n",
            "Epochs = 118........ Training Loss = 12086.5595703125....... Test Loss = 10245.2236328125........ MAE = 76.57936096191406\n",
            "Epochs = 119........ Training Loss = 11885.3349609375....... Test Loss = 10062.443359375........ MAE = 75.7788314819336\n",
            "Epochs = 120........ Training Loss = 11687.236328125....... Test Loss = 9882.72265625........ MAE = 74.99872589111328\n",
            "Epochs = 121........ Training Loss = 11492.3486328125....... Test Loss = 9706.1455078125........ MAE = 74.23704528808594\n",
            "Epochs = 122........ Training Loss = 11300.7548828125....... Test Loss = 9532.794921875........ MAE = 73.49341583251953\n",
            "Epochs = 123........ Training Loss = 11112.533203125....... Test Loss = 9362.755859375........ MAE = 72.76566314697266\n",
            "Epochs = 124........ Training Loss = 10927.7587890625....... Test Loss = 9196.1005859375........ MAE = 72.05970001220703\n",
            "Epochs = 125........ Training Loss = 10746.5....... Test Loss = 9032.8984375........ MAE = 71.3777084350586\n",
            "Epochs = 126........ Training Loss = 10568.82421875....... Test Loss = 8873.21875........ MAE = 70.71485137939453\n",
            "Epochs = 127........ Training Loss = 10394.7890625....... Test Loss = 8717.1201171875........ MAE = 70.07412719726562\n",
            "Epochs = 128........ Training Loss = 10224.4521484375....... Test Loss = 8564.6630859375........ MAE = 69.45858764648438\n",
            "Epochs = 129........ Training Loss = 10057.8603515625....... Test Loss = 8415.8935546875........ MAE = 68.87196350097656\n",
            "Epochs = 130........ Training Loss = 9895.064453125....... Test Loss = 8270.8603515625........ MAE = 68.3165283203125\n",
            "Epochs = 131........ Training Loss = 9736.103515625....... Test Loss = 8129.60595703125........ MAE = 67.78878021240234\n",
            "Epochs = 132........ Training Loss = 9581.01171875....... Test Loss = 7992.16357421875........ MAE = 67.29337310791016\n",
            "Epochs = 133........ Training Loss = 9429.822265625....... Test Loss = 7858.5634765625........ MAE = 66.83025360107422\n",
            "Epochs = 134........ Training Loss = 9282.5576171875....... Test Loss = 7728.82958984375........ MAE = 66.40216064453125\n",
            "Epochs = 135........ Training Loss = 9139.236328125....... Test Loss = 7602.98193359375........ MAE = 66.00425720214844\n",
            "Epochs = 136........ Training Loss = 8999.876953125....... Test Loss = 7481.03271484375........ MAE = 65.63725280761719\n",
            "Epochs = 137........ Training Loss = 8864.486328125....... Test Loss = 7362.990234375........ MAE = 65.30035400390625\n",
            "Epochs = 138........ Training Loss = 8733.0693359375....... Test Loss = 7248.85400390625........ MAE = 64.9913330078125\n",
            "Epochs = 139........ Training Loss = 8605.6220703125....... Test Loss = 7138.623046875........ MAE = 64.70712280273438\n",
            "Epochs = 140........ Training Loss = 8482.1416015625....... Test Loss = 7032.28466796875........ MAE = 64.44905090332031\n",
            "Epochs = 141........ Training Loss = 8362.6123046875....... Test Loss = 6929.826171875........ MAE = 64.21216583251953\n",
            "Epochs = 142........ Training Loss = 8247.0205078125....... Test Loss = 6831.2255859375........ MAE = 63.99497985839844\n",
            "Epochs = 143........ Training Loss = 8135.34228515625....... Test Loss = 6736.45556640625........ MAE = 63.7962646484375\n",
            "Epochs = 144........ Training Loss = 8027.55029296875....... Test Loss = 6645.48486328125........ MAE = 63.61332702636719\n",
            "Epochs = 145........ Training Loss = 7923.61328125....... Test Loss = 6558.2763671875........ MAE = 63.44678497314453\n",
            "Epochs = 146........ Training Loss = 7823.494140625....... Test Loss = 6474.787109375........ MAE = 63.293365478515625\n",
            "Epochs = 147........ Training Loss = 7727.14990234375....... Test Loss = 6394.97119140625........ MAE = 63.15547180175781\n",
            "Epochs = 148........ Training Loss = 7634.53759765625....... Test Loss = 6318.77392578125........ MAE = 63.03208923339844\n",
            "Epochs = 149........ Training Loss = 7545.60400390625....... Test Loss = 6246.14013671875........ MAE = 62.92293167114258\n",
            "Epochs = 150........ Training Loss = 7460.29541015625....... Test Loss = 6177.00634765625........ MAE = 62.8265266418457\n",
            "Epochs = 151........ Training Loss = 7378.5537109375....... Test Loss = 6111.30908203125........ MAE = 62.74092102050781\n",
            "Epochs = 152........ Training Loss = 7300.3154296875....... Test Loss = 6048.9755859375........ MAE = 62.66440963745117\n",
            "Epochs = 153........ Training Loss = 7225.51416015625....... Test Loss = 5989.9345703125........ MAE = 62.59477233886719\n",
            "Epochs = 154........ Training Loss = 7154.08154296875....... Test Loss = 5934.10693359375........ MAE = 62.532867431640625\n",
            "Epochs = 155........ Training Loss = 7085.9443359375....... Test Loss = 5881.412109375........ MAE = 62.47825241088867\n",
            "Epochs = 156........ Training Loss = 7021.02685546875....... Test Loss = 5831.7666015625........ MAE = 62.429996490478516\n",
            "Epochs = 157........ Training Loss = 6959.25048828125....... Test Loss = 5785.0830078125........ MAE = 62.38833236694336\n",
            "Epochs = 158........ Training Loss = 6900.533203125....... Test Loss = 5741.27294921875........ MAE = 62.35348892211914\n",
            "Epochs = 159........ Training Loss = 6844.7939453125....... Test Loss = 5700.24609375........ MAE = 62.32469940185547\n",
            "Epochs = 160........ Training Loss = 6791.94775390625....... Test Loss = 5661.90673828125........ MAE = 62.30143356323242\n",
            "Epochs = 161........ Training Loss = 6741.9052734375....... Test Loss = 5626.162109375........ MAE = 62.283042907714844\n",
            "Epochs = 162........ Training Loss = 6694.580078125....... Test Loss = 5592.91650390625........ MAE = 62.269649505615234\n",
            "Epochs = 163........ Training Loss = 6649.8837890625....... Test Loss = 5562.07177734375........ MAE = 62.261104583740234\n",
            "Epochs = 164........ Training Loss = 6607.72412109375....... Test Loss = 5533.53173828125........ MAE = 62.25864791870117\n",
            "Epochs = 165........ Training Loss = 6568.01220703125....... Test Loss = 5507.19873046875........ MAE = 62.26301956176758\n",
            "Epochs = 166........ Training Loss = 6530.65576171875....... Test Loss = 5482.97314453125........ MAE = 62.27365493774414\n",
            "Epochs = 167........ Training Loss = 6495.56396484375....... Test Loss = 5460.7587890625........ MAE = 62.287960052490234\n",
            "Epochs = 168........ Training Loss = 6462.64501953125....... Test Loss = 5440.45751953125........ MAE = 62.3057861328125\n",
            "Epochs = 169........ Training Loss = 6431.80859375....... Test Loss = 5421.974609375........ MAE = 62.327049255371094\n",
            "Epochs = 170........ Training Loss = 6402.96435546875....... Test Loss = 5405.21337890625........ MAE = 62.35147476196289\n",
            "Epochs = 171........ Training Loss = 6376.02294921875....... Test Loss = 5390.08154296875........ MAE = 62.38079071044922\n",
            "Epochs = 172........ Training Loss = 6350.89599609375....... Test Loss = 5376.4853515625........ MAE = 62.41474151611328\n",
            "Epochs = 173........ Training Loss = 6327.4970703125....... Test Loss = 5364.3349609375........ MAE = 62.454139709472656\n",
            "Epochs = 174........ Training Loss = 6305.7392578125....... Test Loss = 5353.54150390625........ MAE = 62.497047424316406\n",
            "Epochs = 175........ Training Loss = 6285.5400390625....... Test Loss = 5344.0185546875........ MAE = 62.54280090332031\n",
            "Epochs = 176........ Training Loss = 6266.8173828125....... Test Loss = 5335.68212890625........ MAE = 62.59022521972656\n",
            "Epochs = 177........ Training Loss = 6249.48974609375....... Test Loss = 5328.45068359375........ MAE = 62.639915466308594\n",
            "Epochs = 178........ Training Loss = 6233.4794921875....... Test Loss = 5322.2451171875........ MAE = 62.69233703613281\n",
            "Epochs = 179........ Training Loss = 6218.71240234375....... Test Loss = 5316.98876953125........ MAE = 62.74667739868164\n",
            "Epochs = 180........ Training Loss = 6205.11181640625....... Test Loss = 5312.607421875........ MAE = 62.80257797241211\n",
            "Epochs = 181........ Training Loss = 6192.60888671875....... Test Loss = 5309.0322265625........ MAE = 62.857887268066406\n",
            "Epochs = 182........ Training Loss = 6181.1337890625....... Test Loss = 5306.1943359375........ MAE = 62.912353515625\n",
            "Epochs = 183........ Training Loss = 6170.62158203125....... Test Loss = 5304.02880859375........ MAE = 62.965213775634766\n",
            "Epochs = 184........ Training Loss = 6161.00830078125....... Test Loss = 5302.4755859375........ MAE = 63.0163688659668\n",
            "Epochs = 185........ Training Loss = 6152.232421875....... Test Loss = 5301.47509765625........ MAE = 63.065643310546875\n",
            "Epochs = 186........ Training Loss = 6144.236328125....... Test Loss = 5300.97119140625........ MAE = 63.11321258544922\n",
            "Epochs = 187........ Training Loss = 6136.96337890625....... Test Loss = 5300.91259765625........ MAE = 63.15915298461914\n",
            "Epochs = 188........ Training Loss = 6130.36279296875....... Test Loss = 5301.25........ MAE = 63.20329284667969\n",
            "Epochs = 189........ Training Loss = 6124.3828125....... Test Loss = 5301.935546875........ MAE = 63.24568557739258\n",
            "Epochs = 190........ Training Loss = 6118.97607421875....... Test Loss = 5302.92822265625........ MAE = 63.28634262084961\n",
            "Epochs = 191........ Training Loss = 6114.09765625....... Test Loss = 5304.1865234375........ MAE = 63.32535171508789\n",
            "Epochs = 192........ Training Loss = 6109.7060546875....... Test Loss = 5305.67236328125........ MAE = 63.362815856933594\n",
            "Epochs = 193........ Training Loss = 6105.75927734375....... Test Loss = 5307.35009765625........ MAE = 63.39878463745117\n",
            "Epochs = 194........ Training Loss = 6102.22216796875....... Test Loss = 5309.189453125........ MAE = 63.4332160949707\n",
            "Epochs = 195........ Training Loss = 6099.05810546875....... Test Loss = 5311.15869140625........ MAE = 63.466190338134766\n",
            "Epochs = 196........ Training Loss = 6096.2333984375....... Test Loss = 5313.23291015625........ MAE = 63.497737884521484\n",
            "Epochs = 197........ Training Loss = 6093.72021484375....... Test Loss = 5315.38525390625........ MAE = 63.52799987792969\n",
            "Epochs = 198........ Training Loss = 6091.48681640625....... Test Loss = 5317.59521484375........ MAE = 63.55691909790039\n",
            "Epochs = 199........ Training Loss = 6089.50927734375....... Test Loss = 5319.83984375........ MAE = 63.58464050292969\n",
            "Epochs = 200........ Training Loss = 6087.76220703125....... Test Loss = 5322.1025390625........ MAE = 63.61137390136719\n",
            "Epochs = 201........ Training Loss = 6086.22216796875....... Test Loss = 5324.3681640625........ MAE = 63.637229919433594\n",
            "Epochs = 202........ Training Loss = 6084.86865234375....... Test Loss = 5326.61962890625........ MAE = 63.662147521972656\n",
            "Epochs = 203........ Training Loss = 6083.68310546875....... Test Loss = 5328.845703125........ MAE = 63.68629455566406\n",
            "Epochs = 204........ Training Loss = 6082.64697265625....... Test Loss = 5331.0341796875........ MAE = 63.70952224731445\n",
            "Epochs = 205........ Training Loss = 6081.74560546875....... Test Loss = 5333.177734375........ MAE = 63.73188400268555\n",
            "Epochs = 206........ Training Loss = 6080.962890625....... Test Loss = 5335.2646484375........ MAE = 63.753414154052734\n",
            "Epochs = 207........ Training Loss = 6080.28515625....... Test Loss = 5337.29052734375........ MAE = 63.77409362792969\n",
            "Epochs = 208........ Training Loss = 6079.7021484375....... Test Loss = 5339.24853515625........ MAE = 63.7938232421875\n",
            "Epochs = 209........ Training Loss = 6079.2001953125....... Test Loss = 5341.134765625........ MAE = 63.812477111816406\n",
            "Epochs = 210........ Training Loss = 6078.7724609375....... Test Loss = 5342.9443359375........ MAE = 63.83007049560547\n",
            "Epochs = 211........ Training Loss = 6078.40673828125....... Test Loss = 5344.67578125........ MAE = 63.84672927856445\n",
            "Epochs = 212........ Training Loss = 6078.09716796875....... Test Loss = 5346.326171875........ MAE = 63.86252212524414\n",
            "Epochs = 213........ Training Loss = 6077.83544921875....... Test Loss = 5347.89404296875........ MAE = 63.877437591552734\n",
            "Epochs = 214........ Training Loss = 6077.61572265625....... Test Loss = 5349.38037109375........ MAE = 63.89142608642578\n",
            "Epochs = 215........ Training Loss = 6077.43310546875....... Test Loss = 5350.78271484375........ MAE = 63.904502868652344\n",
            "Epochs = 216........ Training Loss = 6077.28125....... Test Loss = 5352.103515625........ MAE = 63.916744232177734\n",
            "Epochs = 217........ Training Loss = 6077.1552734375....... Test Loss = 5353.34326171875........ MAE = 63.92824935913086\n",
            "Epochs = 218........ Training Loss = 6077.05126953125....... Test Loss = 5354.5029296875........ MAE = 63.93900680541992\n",
            "Epochs = 219........ Training Loss = 6076.9677734375....... Test Loss = 5355.58447265625........ MAE = 63.949039459228516\n",
            "Epochs = 220........ Training Loss = 6076.89990234375....... Test Loss = 5356.59033203125........ MAE = 63.9583740234375\n",
            "Epochs = 221........ Training Loss = 6076.84521484375....... Test Loss = 5357.5224609375........ MAE = 63.96701431274414\n",
            "Epochs = 222........ Training Loss = 6076.8017578125....... Test Loss = 5358.38232421875........ MAE = 63.9749641418457\n",
            "Epochs = 223........ Training Loss = 6076.76806640625....... Test Loss = 5359.17431640625........ MAE = 63.98228454589844\n",
            "Epochs = 224........ Training Loss = 6076.7412109375....... Test Loss = 5359.8994140625........ MAE = 63.989044189453125\n",
            "Epochs = 225........ Training Loss = 6076.720703125....... Test Loss = 5360.5625........ MAE = 63.995208740234375\n",
            "Epochs = 226........ Training Loss = 6076.7041015625....... Test Loss = 5361.1640625........ MAE = 64.00080108642578\n",
            "Epochs = 227........ Training Loss = 6076.6923828125....... Test Loss = 5361.708984375........ MAE = 64.0058364868164\n",
            "Epochs = 228........ Training Loss = 6076.6826171875....... Test Loss = 5362.19873046875........ MAE = 64.01034545898438\n",
            "Epochs = 229........ Training Loss = 6076.67578125....... Test Loss = 5362.63720703125........ MAE = 64.0143814086914\n",
            "Epochs = 230........ Training Loss = 6076.6708984375....... Test Loss = 5363.02734375........ MAE = 64.01795196533203\n",
            "Epochs = 231........ Training Loss = 6076.66650390625....... Test Loss = 5363.37060546875........ MAE = 64.02111053466797\n",
            "Epochs = 232........ Training Loss = 6076.6630859375....... Test Loss = 5363.67236328125........ MAE = 64.02388000488281\n",
            "Epochs = 233........ Training Loss = 6076.66015625....... Test Loss = 5363.93310546875........ MAE = 64.02629089355469\n",
            "Epochs = 234........ Training Loss = 6076.65673828125....... Test Loss = 5364.1572265625........ MAE = 64.02837371826172\n",
            "Epochs = 235........ Training Loss = 6076.65478515625....... Test Loss = 5364.34521484375........ MAE = 64.03014373779297\n",
            "Epochs = 236........ Training Loss = 6076.65185546875....... Test Loss = 5364.501953125........ MAE = 64.0316162109375\n",
            "Epochs = 237........ Training Loss = 6076.64892578125....... Test Loss = 5364.62890625........ MAE = 64.03282165527344\n",
            "Epochs = 238........ Training Loss = 6076.64501953125....... Test Loss = 5364.7275390625........ MAE = 64.03376007080078\n",
            "Epochs = 239........ Training Loss = 6076.64208984375....... Test Loss = 5364.80029296875........ MAE = 64.03446960449219\n",
            "Epochs = 240........ Training Loss = 6076.6376953125....... Test Loss = 5364.85205078125........ MAE = 64.03497314453125\n",
            "Epochs = 241........ Training Loss = 6076.6337890625....... Test Loss = 5364.88134765625........ MAE = 64.0352783203125\n",
            "Epochs = 242........ Training Loss = 6076.62939453125....... Test Loss = 5364.89111328125........ MAE = 64.03541564941406\n",
            "Epochs = 243........ Training Loss = 6076.625....... Test Loss = 5364.8837890625........ MAE = 64.0353775024414\n",
            "Epochs = 244........ Training Loss = 6076.62109375....... Test Loss = 5364.861328125........ MAE = 64.03520965576172\n",
            "Epochs = 245........ Training Loss = 6076.61572265625....... Test Loss = 5364.82421875........ MAE = 64.03491973876953\n",
            "Epochs = 246........ Training Loss = 6076.61083984375....... Test Loss = 5364.775390625........ MAE = 64.03450012207031\n",
            "Epochs = 247........ Training Loss = 6076.60546875....... Test Loss = 5364.71435546875........ MAE = 64.03398895263672\n",
            "Epochs = 248........ Training Loss = 6076.6005859375....... Test Loss = 5364.64404296875........ MAE = 64.03338623046875\n",
            "Epochs = 249........ Training Loss = 6076.59521484375....... Test Loss = 5364.56494140625........ MAE = 64.03271484375\n",
            "Epochs = 250........ Training Loss = 6076.59033203125....... Test Loss = 5364.47900390625........ MAE = 64.03196716308594\n",
            "Epochs = 251........ Training Loss = 6076.58544921875....... Test Loss = 5364.38720703125........ MAE = 64.03115844726562\n",
            "Epochs = 252........ Training Loss = 6076.580078125....... Test Loss = 5364.2900390625........ MAE = 64.03030395507812\n",
            "Epochs = 253........ Training Loss = 6076.57568359375....... Test Loss = 5364.1884765625........ MAE = 64.02941131591797\n",
            "Epochs = 254........ Training Loss = 6076.5712890625....... Test Loss = 5364.08349609375........ MAE = 64.02848815917969\n",
            "Epochs = 255........ Training Loss = 6076.56640625....... Test Loss = 5363.9765625........ MAE = 64.02753448486328\n",
            "Epochs = 256........ Training Loss = 6076.5625....... Test Loss = 5363.86669921875........ MAE = 64.02656555175781\n",
            "Epochs = 257........ Training Loss = 6076.55810546875....... Test Loss = 5363.755859375........ MAE = 64.02558135986328\n",
            "Epochs = 258........ Training Loss = 6076.5546875....... Test Loss = 5363.64501953125........ MAE = 64.02458953857422\n",
            "Epochs = 259........ Training Loss = 6076.55078125....... Test Loss = 5363.53369140625........ MAE = 64.02359771728516\n",
            "Epochs = 260........ Training Loss = 6076.54736328125....... Test Loss = 5363.42333984375........ MAE = 64.02261352539062\n",
            "Epochs = 261........ Training Loss = 6076.5439453125....... Test Loss = 5363.3125........ MAE = 64.02162170410156\n",
            "Epochs = 262........ Training Loss = 6076.54150390625....... Test Loss = 5363.20458984375........ MAE = 64.0206527709961\n",
            "Epochs = 263........ Training Loss = 6076.53857421875....... Test Loss = 5363.09765625........ MAE = 64.01969146728516\n",
            "Epochs = 264........ Training Loss = 6076.53564453125....... Test Loss = 5362.99169921875........ MAE = 64.01874542236328\n",
            "Epochs = 265........ Training Loss = 6076.53271484375....... Test Loss = 5362.8896484375........ MAE = 64.01781463623047\n",
            "Epochs = 266........ Training Loss = 6076.53076171875....... Test Loss = 5362.7890625........ MAE = 64.01691436767578\n",
            "Epochs = 267........ Training Loss = 6076.5283203125....... Test Loss = 5362.69091796875........ MAE = 64.01602172851562\n",
            "Epochs = 268........ Training Loss = 6076.5263671875....... Test Loss = 5362.595703125........ MAE = 64.01517486572266\n",
            "Epochs = 269........ Training Loss = 6076.52490234375....... Test Loss = 5362.50390625........ MAE = 64.01433563232422\n",
            "Epochs = 270........ Training Loss = 6076.5234375....... Test Loss = 5362.4150390625........ MAE = 64.01354217529297\n",
            "Epochs = 271........ Training Loss = 6076.521484375....... Test Loss = 5362.3291015625........ MAE = 64.01275634765625\n",
            "Epochs = 272........ Training Loss = 6076.5205078125....... Test Loss = 5362.24609375........ MAE = 64.01200866699219\n",
            "Epochs = 273........ Training Loss = 6076.51953125....... Test Loss = 5362.1669921875........ MAE = 64.01128387451172\n",
            "Epochs = 274........ Training Loss = 6076.51708984375....... Test Loss = 5362.09130859375........ MAE = 64.0105972290039\n",
            "Epochs = 275........ Training Loss = 6076.5166015625....... Test Loss = 5362.01806640625........ MAE = 64.00993347167969\n",
            "Epochs = 276........ Training Loss = 6076.515625....... Test Loss = 5361.94873046875........ MAE = 64.00930786132812\n",
            "Epochs = 277........ Training Loss = 6076.513671875....... Test Loss = 5361.88232421875........ MAE = 64.00870513916016\n",
            "Epochs = 278........ Training Loss = 6076.513671875....... Test Loss = 5361.8193359375........ MAE = 64.00814056396484\n",
            "Epochs = 279........ Training Loss = 6076.51220703125....... Test Loss = 5361.759765625........ MAE = 64.0075912475586\n",
            "Epochs = 280........ Training Loss = 6076.5126953125....... Test Loss = 5361.70361328125........ MAE = 64.007080078125\n",
            "Epochs = 281........ Training Loss = 6076.51171875....... Test Loss = 5361.65087890625........ MAE = 64.00659942626953\n",
            "Epochs = 282........ Training Loss = 6076.51123046875....... Test Loss = 5361.6005859375........ MAE = 64.00614166259766\n",
            "Epochs = 283........ Training Loss = 6076.51025390625....... Test Loss = 5361.552734375........ MAE = 64.00570678710938\n",
            "Epochs = 284........ Training Loss = 6076.51025390625....... Test Loss = 5361.5087890625........ MAE = 64.00530242919922\n",
            "Epochs = 285........ Training Loss = 6076.509765625....... Test Loss = 5361.46728515625........ MAE = 64.00492095947266\n",
            "Epochs = 286........ Training Loss = 6076.5087890625....... Test Loss = 5361.4287109375........ MAE = 64.00457000732422\n",
            "Epochs = 287........ Training Loss = 6076.50830078125....... Test Loss = 5361.39208984375........ MAE = 64.0042495727539\n",
            "Epochs = 288........ Training Loss = 6076.50830078125....... Test Loss = 5361.35888671875........ MAE = 64.00394439697266\n",
            "Epochs = 289........ Training Loss = 6076.50830078125....... Test Loss = 5361.32763671875........ MAE = 64.00365447998047\n",
            "Epochs = 290........ Training Loss = 6076.5078125....... Test Loss = 5361.298828125........ MAE = 64.00340270996094\n",
            "Epochs = 291........ Training Loss = 6076.5078125....... Test Loss = 5361.2724609375........ MAE = 64.00315856933594\n",
            "Epochs = 292........ Training Loss = 6076.50732421875....... Test Loss = 5361.24853515625........ MAE = 64.0029296875\n",
            "Epochs = 293........ Training Loss = 6076.5068359375....... Test Loss = 5361.22607421875........ MAE = 64.00273132324219\n",
            "Epochs = 294........ Training Loss = 6076.5068359375....... Test Loss = 5361.2060546875........ MAE = 64.00254821777344\n",
            "Epochs = 295........ Training Loss = 6076.5068359375....... Test Loss = 5361.1875........ MAE = 64.00238800048828\n",
            "Epochs = 296........ Training Loss = 6076.50634765625....... Test Loss = 5361.1708984375........ MAE = 64.00223541259766\n",
            "Epochs = 297........ Training Loss = 6076.50634765625....... Test Loss = 5361.15576171875........ MAE = 64.0020980834961\n",
            "Epochs = 298........ Training Loss = 6076.505859375....... Test Loss = 5361.14306640625........ MAE = 64.0019760131836\n",
            "Epochs = 299........ Training Loss = 6076.505859375....... Test Loss = 5361.13134765625........ MAE = 64.00186920166016\n",
            "Epochs = 300........ Training Loss = 6076.505859375....... Test Loss = 5361.12060546875........ MAE = 64.00177764892578\n",
            "Epochs = 301........ Training Loss = 6076.505859375....... Test Loss = 5361.111328125........ MAE = 64.00169372558594\n",
            "Epochs = 302........ Training Loss = 6076.5048828125....... Test Loss = 5361.1044921875........ MAE = 64.00162506103516\n",
            "Epochs = 303........ Training Loss = 6076.5048828125....... Test Loss = 5361.09716796875........ MAE = 64.0015640258789\n",
            "Epochs = 304........ Training Loss = 6076.5048828125....... Test Loss = 5361.091796875........ MAE = 64.00151824951172\n",
            "Epochs = 305........ Training Loss = 6076.5048828125....... Test Loss = 5361.08740234375........ MAE = 64.00147247314453\n",
            "Epochs = 306........ Training Loss = 6076.5048828125....... Test Loss = 5361.08251953125........ MAE = 64.00144958496094\n",
            "Epochs = 307........ Training Loss = 6076.50390625....... Test Loss = 5361.080078125........ MAE = 64.00141906738281\n",
            "Epochs = 308........ Training Loss = 6076.50439453125....... Test Loss = 5361.07861328125........ MAE = 64.00139617919922\n",
            "Epochs = 309........ Training Loss = 6076.50390625....... Test Loss = 5361.0771484375........ MAE = 64.00138854980469\n",
            "Epochs = 310........ Training Loss = 6076.50390625....... Test Loss = 5361.076171875........ MAE = 64.00138092041016\n",
            "Epochs = 311........ Training Loss = 6076.50390625....... Test Loss = 5361.076171875........ MAE = 64.00138854980469\n",
            "Epochs = 312........ Training Loss = 6076.50341796875....... Test Loss = 5361.07666015625........ MAE = 64.00139617919922\n",
            "Epochs = 313........ Training Loss = 6076.50341796875....... Test Loss = 5361.07763671875........ MAE = 64.00140380859375\n",
            "Epochs = 314........ Training Loss = 6076.50341796875....... Test Loss = 5361.07958984375........ MAE = 64.00141906738281\n",
            "Epochs = 315........ Training Loss = 6076.5029296875....... Test Loss = 5361.0810546875........ MAE = 64.00143432617188\n",
            "Epochs = 316........ Training Loss = 6076.5029296875....... Test Loss = 5361.08251953125........ MAE = 64.00145721435547\n",
            "Epochs = 317........ Training Loss = 6076.5029296875....... Test Loss = 5361.0859375........ MAE = 64.00148010253906\n",
            "Epochs = 318........ Training Loss = 6076.50244140625....... Test Loss = 5361.087890625........ MAE = 64.00151062011719\n",
            "Epochs = 319........ Training Loss = 6076.50244140625....... Test Loss = 5361.09130859375........ MAE = 64.00153350830078\n",
            "Epochs = 320........ Training Loss = 6076.50244140625....... Test Loss = 5361.09375........ MAE = 64.0015640258789\n",
            "Epochs = 321........ Training Loss = 6076.50244140625....... Test Loss = 5361.0966796875........ MAE = 64.00159454345703\n",
            "Epochs = 322........ Training Loss = 6076.501953125....... Test Loss = 5361.10107421875........ MAE = 64.00162506103516\n",
            "Epochs = 323........ Training Loss = 6076.501953125....... Test Loss = 5361.1044921875........ MAE = 64.00166320800781\n",
            "Epochs = 324........ Training Loss = 6076.50244140625....... Test Loss = 5361.107421875........ MAE = 64.00169372558594\n",
            "Epochs = 325........ Training Loss = 6076.501953125....... Test Loss = 5361.111328125........ MAE = 64.0017318725586\n",
            "Epochs = 326........ Training Loss = 6076.501953125....... Test Loss = 5361.115234375........ MAE = 64.00177001953125\n",
            "Epochs = 327........ Training Loss = 6076.50146484375....... Test Loss = 5361.1181640625........ MAE = 64.00180053710938\n",
            "Epochs = 328........ Training Loss = 6076.501953125....... Test Loss = 5361.1220703125........ MAE = 64.0018310546875\n",
            "Epochs = 329........ Training Loss = 6076.50146484375....... Test Loss = 5361.12548828125........ MAE = 64.00186157226562\n",
            "Epochs = 330........ Training Loss = 6076.5009765625....... Test Loss = 5361.12841796875........ MAE = 64.00189971923828\n",
            "Epochs = 331........ Training Loss = 6076.5009765625....... Test Loss = 5361.1328125........ MAE = 64.0019302368164\n",
            "Epochs = 332........ Training Loss = 6076.50048828125....... Test Loss = 5361.13623046875........ MAE = 64.00196838378906\n",
            "Epochs = 333........ Training Loss = 6076.5009765625....... Test Loss = 5361.13916015625........ MAE = 64.00199127197266\n",
            "Epochs = 334........ Training Loss = 6076.50048828125....... Test Loss = 5361.14208984375........ MAE = 64.00202178955078\n",
            "Epochs = 335........ Training Loss = 6076.5009765625....... Test Loss = 5361.14599609375........ MAE = 64.00205993652344\n",
            "Epochs = 336........ Training Loss = 6076.5....... Test Loss = 5361.14892578125........ MAE = 64.00209045410156\n",
            "Epochs = 337........ Training Loss = 6076.5009765625....... Test Loss = 5361.1513671875........ MAE = 64.00211334228516\n",
            "Epochs = 338........ Training Loss = 6076.50048828125....... Test Loss = 5361.15478515625........ MAE = 64.00213623046875\n",
            "Epochs = 339........ Training Loss = 6076.5....... Test Loss = 5361.1572265625........ MAE = 64.00216674804688\n",
            "Epochs = 340........ Training Loss = 6076.50048828125....... Test Loss = 5361.15966796875........ MAE = 64.002197265625\n",
            "Epochs = 341........ Training Loss = 6076.5....... Test Loss = 5361.162109375........ MAE = 64.0022201538086\n",
            "Epochs = 342........ Training Loss = 6076.5....... Test Loss = 5361.16455078125........ MAE = 64.00224304199219\n",
            "Epochs = 343........ Training Loss = 6076.4990234375....... Test Loss = 5361.1669921875........ MAE = 64.00226593017578\n",
            "Epochs = 344........ Training Loss = 6076.50048828125....... Test Loss = 5361.16943359375........ MAE = 64.00228881835938\n",
            "Epochs = 345........ Training Loss = 6076.5....... Test Loss = 5361.1708984375........ MAE = 64.0022964477539\n",
            "Epochs = 346........ Training Loss = 6076.4990234375....... Test Loss = 5361.1728515625........ MAE = 64.0023193359375\n",
            "Epochs = 347........ Training Loss = 6076.50048828125....... Test Loss = 5361.1748046875........ MAE = 64.0023422241211\n",
            "Epochs = 348........ Training Loss = 6076.4990234375....... Test Loss = 5361.17626953125........ MAE = 64.00235748291016\n",
            "Epochs = 349........ Training Loss = 6076.5....... Test Loss = 5361.1787109375........ MAE = 64.00237274169922\n",
            "Epochs = 350........ Training Loss = 6076.49853515625....... Test Loss = 5361.18017578125........ MAE = 64.00238037109375\n",
            "Epochs = 351........ Training Loss = 6076.49853515625....... Test Loss = 5361.18115234375........ MAE = 64.00240325927734\n",
            "Epochs = 352........ Training Loss = 6076.4990234375....... Test Loss = 5361.18212890625........ MAE = 64.00240325927734\n",
            "Epochs = 353........ Training Loss = 6076.49853515625....... Test Loss = 5361.18359375........ MAE = 64.00242614746094\n",
            "Epochs = 354........ Training Loss = 6076.4990234375....... Test Loss = 5361.1845703125........ MAE = 64.00244140625\n",
            "Epochs = 355........ Training Loss = 6076.49853515625....... Test Loss = 5361.185546875........ MAE = 64.00244903564453\n",
            "Epochs = 356........ Training Loss = 6076.49853515625....... Test Loss = 5361.1865234375........ MAE = 64.00245666503906\n",
            "Epochs = 357........ Training Loss = 6076.498046875....... Test Loss = 5361.18701171875........ MAE = 64.0024642944336\n",
            "Epochs = 358........ Training Loss = 6076.498046875....... Test Loss = 5361.1884765625........ MAE = 64.00247192382812\n",
            "Epochs = 359........ Training Loss = 6076.49853515625....... Test Loss = 5361.18896484375........ MAE = 64.00247955322266\n",
            "Epochs = 360........ Training Loss = 6076.49853515625....... Test Loss = 5361.189453125........ MAE = 64.00247955322266\n",
            "Epochs = 361........ Training Loss = 6076.49853515625....... Test Loss = 5361.18994140625........ MAE = 64.00249481201172\n",
            "Epochs = 362........ Training Loss = 6076.49853515625....... Test Loss = 5361.1904296875........ MAE = 64.00249481201172\n",
            "Epochs = 363........ Training Loss = 6076.498046875....... Test Loss = 5361.19140625........ MAE = 64.00250244140625\n",
            "Epochs = 364........ Training Loss = 6076.498046875....... Test Loss = 5361.19189453125........ MAE = 64.00250244140625\n",
            "Epochs = 365........ Training Loss = 6076.498046875....... Test Loss = 5361.19189453125........ MAE = 64.00251007080078\n",
            "Epochs = 366........ Training Loss = 6076.49755859375....... Test Loss = 5361.19189453125........ MAE = 64.00251007080078\n",
            "Epochs = 367........ Training Loss = 6076.498046875....... Test Loss = 5361.19189453125........ MAE = 64.00251770019531\n",
            "Epochs = 368........ Training Loss = 6076.49755859375....... Test Loss = 5361.1923828125........ MAE = 64.00251007080078\n",
            "Epochs = 369........ Training Loss = 6076.49755859375....... Test Loss = 5361.1923828125........ MAE = 64.00251770019531\n",
            "Epochs = 370........ Training Loss = 6076.4970703125....... Test Loss = 5361.1923828125........ MAE = 64.00251007080078\n",
            "Epochs = 371........ Training Loss = 6076.49755859375....... Test Loss = 5361.19287109375........ MAE = 64.00251770019531\n",
            "Epochs = 372........ Training Loss = 6076.49755859375....... Test Loss = 5361.1923828125........ MAE = 64.00252532958984\n",
            "Epochs = 373........ Training Loss = 6076.4970703125....... Test Loss = 5361.1923828125........ MAE = 64.00251770019531\n",
            "Epochs = 374........ Training Loss = 6076.49755859375....... Test Loss = 5361.1923828125........ MAE = 64.00251770019531\n",
            "Epochs = 375........ Training Loss = 6076.49755859375....... Test Loss = 5361.19287109375........ MAE = 64.00252532958984\n",
            "Epochs = 376........ Training Loss = 6076.49755859375....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 377........ Training Loss = 6076.49755859375....... Test Loss = 5361.19189453125........ MAE = 64.00251770019531\n",
            "Epochs = 378........ Training Loss = 6076.4970703125....... Test Loss = 5361.1923828125........ MAE = 64.00252532958984\n",
            "Epochs = 379........ Training Loss = 6076.4970703125....... Test Loss = 5361.1923828125........ MAE = 64.00252532958984\n",
            "Epochs = 380........ Training Loss = 6076.49755859375....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 381........ Training Loss = 6076.49755859375....... Test Loss = 5361.1923828125........ MAE = 64.00252532958984\n",
            "Epochs = 382........ Training Loss = 6076.49658203125....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 383........ Training Loss = 6076.49755859375....... Test Loss = 5361.1923828125........ MAE = 64.00253295898438\n",
            "Epochs = 384........ Training Loss = 6076.49755859375....... Test Loss = 5361.1923828125........ MAE = 64.00252532958984\n",
            "Epochs = 385........ Training Loss = 6076.49658203125....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 386........ Training Loss = 6076.49658203125....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 387........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 388........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 389........ Training Loss = 6076.49658203125....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 390........ Training Loss = 6076.4970703125....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 391........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 392........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 393........ Training Loss = 6076.4970703125....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 394........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 395........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 396........ Training Loss = 6076.49609375....... Test Loss = 5361.1923828125........ MAE = 64.00253295898438\n",
            "Epochs = 397........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 398........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 399........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00252532958984\n",
            "Epochs = 400........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 401........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.0025405883789\n",
            "Epochs = 402........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 403........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 404........ Training Loss = 6076.49658203125....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 405........ Training Loss = 6076.49658203125....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 406........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 407........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 408........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.0025405883789\n",
            "Epochs = 409........ Training Loss = 6076.49560546875....... Test Loss = 5361.1923828125........ MAE = 64.00253295898438\n",
            "Epochs = 410........ Training Loss = 6076.49560546875....... Test Loss = 5361.19140625........ MAE = 64.00253295898438\n",
            "Epochs = 411........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.0025405883789\n",
            "Epochs = 412........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 413........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.0025405883789\n",
            "Epochs = 414........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 415........ Training Loss = 6076.49609375....... Test Loss = 5361.19189453125........ MAE = 64.0025405883789\n",
            "Epochs = 416........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 417........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 418........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.0025405883789\n",
            "Epochs = 419........ Training Loss = 6076.4951171875....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 420........ Training Loss = 6076.4951171875....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 421........ Training Loss = 6076.4951171875....... Test Loss = 5361.19189453125........ MAE = 64.00253295898438\n",
            "Epochs = 422........ Training Loss = 6076.4951171875....... Test Loss = 5361.19189453125........ MAE = 64.0025405883789\n",
            "Epochs = 423........ Training Loss = 6076.49560546875....... Test Loss = 5361.19140625........ MAE = 64.0025405883789\n",
            "Epochs = 424........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00254821777344\n",
            "Epochs = 425........ Training Loss = 6076.49560546875....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 426........ Training Loss = 6076.49560546875....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 427........ Training Loss = 6076.4951171875....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 428........ Training Loss = 6076.49560546875....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 429........ Training Loss = 6076.49560546875....... Test Loss = 5361.19189453125........ MAE = 64.00254821777344\n",
            "Epochs = 430........ Training Loss = 6076.49462890625....... Test Loss = 5361.19189453125........ MAE = 64.00254821777344\n",
            "Epochs = 431........ Training Loss = 6076.49462890625....... Test Loss = 5361.19189453125........ MAE = 64.00254821777344\n",
            "Epochs = 432........ Training Loss = 6076.49462890625....... Test Loss = 5361.19189453125........ MAE = 64.00254821777344\n",
            "Epochs = 433........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 434........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 435........ Training Loss = 6076.4951171875....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 436........ Training Loss = 6076.49462890625....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 437........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 438........ Training Loss = 6076.49462890625....... Test Loss = 5361.19189453125........ MAE = 64.00255584716797\n",
            "Epochs = 439........ Training Loss = 6076.494140625....... Test Loss = 5361.19189453125........ MAE = 64.00254821777344\n",
            "Epochs = 440........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.00254821777344\n",
            "Epochs = 441........ Training Loss = 6076.49462890625....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 442........ Training Loss = 6076.49462890625....... Test Loss = 5361.19189453125........ MAE = 64.00254821777344\n",
            "Epochs = 443........ Training Loss = 6076.49462890625....... Test Loss = 5361.19189453125........ MAE = 64.00255584716797\n",
            "Epochs = 444........ Training Loss = 6076.49462890625....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 445........ Training Loss = 6076.49365234375....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 446........ Training Loss = 6076.49365234375....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 447........ Training Loss = 6076.49365234375....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 448........ Training Loss = 6076.49365234375....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 449........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 450........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 451........ Training Loss = 6076.494140625....... Test Loss = 5361.19189453125........ MAE = 64.00255584716797\n",
            "Epochs = 452........ Training Loss = 6076.494140625....... Test Loss = 5361.19189453125........ MAE = 64.00255584716797\n",
            "Epochs = 453........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.0025634765625\n",
            "Epochs = 454........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.0025634765625\n",
            "Epochs = 455........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.0025634765625\n",
            "Epochs = 456........ Training Loss = 6076.49365234375....... Test Loss = 5361.1923828125........ MAE = 64.00255584716797\n",
            "Epochs = 457........ Training Loss = 6076.49365234375....... Test Loss = 5361.19287109375........ MAE = 64.0025634765625\n",
            "Epochs = 458........ Training Loss = 6076.494140625....... Test Loss = 5361.19287109375........ MAE = 64.0025634765625\n",
            "Epochs = 459........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.0025634765625\n",
            "Epochs = 460........ Training Loss = 6076.494140625....... Test Loss = 5361.1923828125........ MAE = 64.0025634765625\n",
            "Epochs = 461........ Training Loss = 6076.494140625....... Test Loss = 5361.19287109375........ MAE = 64.0025634765625\n",
            "Epochs = 462........ Training Loss = 6076.494140625....... Test Loss = 5361.19287109375........ MAE = 64.0025634765625\n",
            "Epochs = 463........ Training Loss = 6076.49365234375....... Test Loss = 5361.19287109375........ MAE = 64.0025634765625\n",
            "Epochs = 464........ Training Loss = 6076.49365234375....... Test Loss = 5361.1923828125........ MAE = 64.0025634765625\n",
            "Epochs = 465........ Training Loss = 6076.49365234375....... Test Loss = 5361.19287109375........ MAE = 64.0025634765625\n",
            "Epochs = 466........ Training Loss = 6076.4931640625....... Test Loss = 5361.1923828125........ MAE = 64.00257110595703\n",
            "Epochs = 467........ Training Loss = 6076.4931640625....... Test Loss = 5361.1923828125........ MAE = 64.0025634765625\n",
            "Epochs = 468........ Training Loss = 6076.4931640625....... Test Loss = 5361.19287109375........ MAE = 64.0025634765625\n",
            "Epochs = 469........ Training Loss = 6076.4931640625....... Test Loss = 5361.193359375........ MAE = 64.0025634765625\n",
            "Epochs = 470........ Training Loss = 6076.49365234375....... Test Loss = 5361.1923828125........ MAE = 64.00257110595703\n",
            "Epochs = 471........ Training Loss = 6076.4931640625....... Test Loss = 5361.193359375........ MAE = 64.00257110595703\n",
            "Epochs = 472........ Training Loss = 6076.4931640625....... Test Loss = 5361.19287109375........ MAE = 64.00257110595703\n",
            "Epochs = 473........ Training Loss = 6076.49365234375....... Test Loss = 5361.193359375........ MAE = 64.0025634765625\n",
            "Epochs = 474........ Training Loss = 6076.49365234375....... Test Loss = 5361.19287109375........ MAE = 64.00257110595703\n",
            "Epochs = 475........ Training Loss = 6076.4931640625....... Test Loss = 5361.193359375........ MAE = 64.00257110595703\n",
            "Epochs = 476........ Training Loss = 6076.49365234375....... Test Loss = 5361.193359375........ MAE = 64.00257873535156\n",
            "Epochs = 477........ Training Loss = 6076.49267578125....... Test Loss = 5361.193359375........ MAE = 64.00257873535156\n",
            "Epochs = 478........ Training Loss = 6076.4931640625....... Test Loss = 5361.193359375........ MAE = 64.00257873535156\n",
            "Epochs = 479........ Training Loss = 6076.4931640625....... Test Loss = 5361.19384765625........ MAE = 64.00257873535156\n",
            "Epochs = 480........ Training Loss = 6076.49267578125....... Test Loss = 5361.19384765625........ MAE = 64.00257873535156\n",
            "Epochs = 481........ Training Loss = 6076.49267578125....... Test Loss = 5361.19384765625........ MAE = 64.00257873535156\n",
            "Epochs = 482........ Training Loss = 6076.49267578125....... Test Loss = 5361.19384765625........ MAE = 64.00257873535156\n",
            "Epochs = 483........ Training Loss = 6076.49267578125....... Test Loss = 5361.19384765625........ MAE = 64.0025863647461\n",
            "Epochs = 484........ Training Loss = 6076.49267578125....... Test Loss = 5361.19384765625........ MAE = 64.0025863647461\n",
            "Epochs = 485........ Training Loss = 6076.49267578125....... Test Loss = 5361.193359375........ MAE = 64.0025863647461\n",
            "Epochs = 486........ Training Loss = 6076.49267578125....... Test Loss = 5361.193359375........ MAE = 64.0025863647461\n",
            "Epochs = 487........ Training Loss = 6076.49267578125....... Test Loss = 5361.19384765625........ MAE = 64.0025863647461\n",
            "Epochs = 488........ Training Loss = 6076.4921875....... Test Loss = 5361.19384765625........ MAE = 64.0025863647461\n",
            "Epochs = 489........ Training Loss = 6076.49267578125....... Test Loss = 5361.19384765625........ MAE = 64.00259399414062\n",
            "Epochs = 490........ Training Loss = 6076.49267578125....... Test Loss = 5361.1943359375........ MAE = 64.00259399414062\n",
            "Epochs = 491........ Training Loss = 6076.49267578125....... Test Loss = 5361.1943359375........ MAE = 64.0025863647461\n",
            "Epochs = 492........ Training Loss = 6076.49267578125....... Test Loss = 5361.1943359375........ MAE = 64.0025863647461\n",
            "Epochs = 493........ Training Loss = 6076.49267578125....... Test Loss = 5361.19482421875........ MAE = 64.00259399414062\n",
            "Epochs = 494........ Training Loss = 6076.49267578125....... Test Loss = 5361.1943359375........ MAE = 64.00259399414062\n",
            "Epochs = 495........ Training Loss = 6076.49267578125....... Test Loss = 5361.1943359375........ MAE = 64.00259399414062\n",
            "Epochs = 496........ Training Loss = 6076.4921875....... Test Loss = 5361.19482421875........ MAE = 64.00259399414062\n",
            "Epochs = 497........ Training Loss = 6076.49267578125....... Test Loss = 5361.19482421875........ MAE = 64.00259399414062\n",
            "Epochs = 498........ Training Loss = 6076.49267578125....... Test Loss = 5361.19482421875........ MAE = 64.00259399414062\n",
            "Epochs = 499........ Training Loss = 6076.49267578125....... Test Loss = 5361.19482421875........ MAE = 64.00260162353516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zGkGQopD36o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sn4msOVjD399"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oc2NeCvBD4AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.from_numpy(X_train)\n",
        "X_test = torch.from_numpy(X_test)"
      ],
      "metadata": {
        "id": "BZzwSAhz-f1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "cqFaitv3-9cH",
        "outputId": "2ea5d328-e00e-4350-84a6-269bde40fdf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-5fa1de14257e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6nBesQsBwcu",
        "outputId": "75fe6ac0-4d7c-4506-ed81-5419f1ef8c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class dib_ref(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = nn.Linear(X_train.shape[1],16)\n",
        "    self.layer_2 = nn.Linear(16,32)\n",
        "    self.layer_3 = nn.Linear(32,16)\n",
        "    self.layer_4 = nn.Linear(16,1)\n",
        "  def forward(self,x):\n",
        "    x = self.layer_1(X_train)\n",
        "    x = self.layer_2(x)\n",
        "    x = self.layer_3(x)\n",
        "    x = self.layer_4(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "vN7la2eM--jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_model = dib_ref()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params=regression_model.parameters(),lr=0.1)"
      ],
      "metadata": {
        "id": "S8ZKuVOWCdzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "  regression_model.train()\n",
        "  y_hat = regression_model(X_train)\n",
        "  loss = loss_fn(y_hat,y_train)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  with torch.inference_mode():\n",
        "    y_pred = regression_model(X_test)\n",
        "    loss2 = loss_fn(y_pred,y_test)\n",
        "    mae = nn.L1Loss()(y_pred, y_test)\n",
        "    print(f'Epoch = {epoch},training loss = {loss}, loss = {loss2}, MAE = {mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlv9nsRjF3iZ",
        "outputId": "d80d6d08-94a5-4663-a98f-445e053fcc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([353])) that is different to the input size (torch.Size([353, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([89])) that is different to the input size (torch.Size([353, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([89])) that is different to the input size (torch.Size([353, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch = 0,training loss = 29794.56640625, loss = 26211.37109375, MAE = 144.61402893066406\n",
            "Epoch = 1,training loss = 29355.619140625, loss = 23954.107421875, MAE = 136.5858612060547\n",
            "Epoch = 2,training loss = 26970.52734375, loss = 18776.021484375, MAE = 116.06907653808594\n",
            "Epoch = 3,training loss = 21465.763671875, loss = 9922.736328125, MAE = 75.19347381591797\n",
            "Epoch = 4,training loss = 11840.611328125, loss = 6895.25537109375, MAE = 72.31130981445312\n",
            "Epoch = 5,training loss = 7140.310546875, loss = 16117.3544921875, MAE = 109.82131958007812\n",
            "Epoch = 6,training loss = 15317.169921875, loss = 7223.71533203125, MAE = 74.12931060791016\n",
            "Epoch = 7,training loss = 7376.73974609375, loss = 5530.361328125, MAE = 62.28668212890625\n",
            "Epoch = 8,training loss = 6604.75146484375, loss = 7406.08056640625, MAE = 65.33914947509766\n",
            "Epoch = 9,training loss = 8976.2841796875, loss = 8422.671875, MAE = 68.7603988647461\n",
            "Epoch = 10,training loss = 10151.888671875, loss = 7954.01220703125, MAE = 67.07658386230469\n",
            "Epoch = 11,training loss = 9612.4111328125, loss = 6504.54541015625, MAE = 63.38602828979492\n",
            "Epoch = 12,training loss = 7890.41748046875, loss = 5372.69580078125, MAE = 62.89610290527344\n",
            "Epoch = 13,training loss = 6276.44921875, loss = 6627.953125, MAE = 71.1219253540039\n",
            "Epoch = 14,training loss = 6907.455078125, loss = 8457.8076171875, MAE = 79.81653594970703\n",
            "Epoch = 15,training loss = 8416.513671875, loss = 7008.25048828125, MAE = 73.05762481689453\n",
            "Epoch = 16,training loss = 7203.24658203125, loss = 5449.7919921875, MAE = 64.56222534179688\n",
            "Epoch = 17,training loss = 6120.11669921875, loss = 5530.9365234375, MAE = 62.29610824584961\n",
            "Epoch = 18,training loss = 6604.603515625, loss = 6051.40576171875, MAE = 62.656532287597656\n",
            "Epoch = 19,training loss = 7326.16259765625, loss = 6038.1123046875, MAE = 62.63857650756836\n",
            "Epoch = 20,training loss = 7309.60986328125, loss = 5569.4248046875, MAE = 62.2655029296875\n",
            "Epoch = 21,training loss = 6667.41015625, loss = 5320.93505859375, MAE = 63.412384033203125\n",
            "Epoch = 22,training loss = 6120.30029296875, loss = 5865.53173828125, MAE = 67.11619567871094\n",
            "Epoch = 23,training loss = 6335.0302734375, loss = 6604.11181640625, MAE = 71.0544204711914\n",
            "Epoch = 24,training loss = 6875.90966796875, loss = 6385.44580078125, MAE = 69.92498779296875\n",
            "Epoch = 25,training loss = 6708.04541015625, loss = 5638.6171875, MAE = 65.82300567626953\n",
            "Epoch = 26,training loss = 6195.38671875, loss = 5315.044921875, MAE = 63.27469253540039\n",
            "Epoch = 27,training loss = 6134.80517578125, loss = 5410.68408203125, MAE = 62.3546257019043\n",
            "Epoch = 28,training loss = 6413.3154296875, loss = 5504.177734375, MAE = 62.25456619262695\n",
            "Epoch = 29,training loss = 6570.60107421875, loss = 5417.22509765625, MAE = 62.310157775878906\n",
            "Epoch = 30,training loss = 6429.40625, loss = 5303.38134765625, MAE = 62.99308395385742\n",
            "Epoch = 31,training loss = 6165.5634765625, loss = 5428.1123046875, MAE = 64.52049255371094\n",
            "Epoch = 32,training loss = 6091.71240234375, loss = 5778.7314453125, MAE = 66.62625122070312\n",
            "Epoch = 33,training loss = 6274.1572265625, loss = 5941.48388671875, MAE = 67.53515625\n",
            "Epoch = 34,training loss = 6382.2177734375, loss = 5711.53466796875, MAE = 66.25706481933594\n",
            "Epoch = 35,training loss = 6232.5751953125, loss = 5409.599609375, MAE = 64.37857818603516\n",
            "Epoch = 36,training loss = 6087.2109375, loss = 5302.38427734375, MAE = 63.21040344238281\n",
            "Epoch = 37,training loss = 6130.322265625, loss = 5323.078125, MAE = 62.65947723388672\n",
            "Epoch = 38,training loss = 6239.63427734375, loss = 5327.10791015625, MAE = 62.62220764160156\n",
            "Epoch = 39,training loss = 6250.392578125, loss = 5301.31787109375, MAE = 63.04547119140625\n",
            "Epoch = 40,training loss = 6155.20703125, loss = 5345.02978515625, MAE = 63.8430290222168\n",
            "Epoch = 41,training loss = 6080.3359375, loss = 5502.96044921875, MAE = 65.0611343383789\n",
            "Epoch = 42,training loss = 6117.7861328125, loss = 5640.724609375, MAE = 65.88040924072266\n",
            "Epoch = 43,training loss = 6188.3984375, loss = 5601.97802734375, MAE = 65.66316223144531\n",
            "Epoch = 44,training loss = 6166.90869140625, loss = 5445.2939453125, MAE = 64.65705108642578\n",
            "Epoch = 45,training loss = 6095.0517578125, loss = 5330.86328125, MAE = 63.703224182128906\n",
            "Epoch = 46,training loss = 6083.9990234375, loss = 5300.39453125, MAE = 63.2080078125\n",
            "Epoch = 47,training loss = 6128.69140625, loss = 5299.6171875, MAE = 63.07151794433594\n",
            "Epoch = 48,training loss = 6149.4052734375, loss = 5301.82373046875, MAE = 63.29217529296875\n",
            "Epoch = 49,training loss = 6116.86962890625, loss = 5338.61572265625, MAE = 63.78759002685547\n",
            "Epoch = 50,training loss = 6079.7509765625, loss = 5427.61279296875, MAE = 64.5290298461914\n",
            "Epoch = 51,training loss = 6088.4033203125, loss = 5506.94775390625, MAE = 65.09627532958984\n",
            "Epoch = 52,training loss = 6118.73974609375, loss = 5497.90966796875, MAE = 65.03523254394531\n",
            "Epoch = 53,training loss = 6114.794921875, loss = 5416.86669921875, MAE = 64.44772338867188\n",
            "Epoch = 54,training loss = 6085.50732421875, loss = 5342.6767578125, MAE = 63.826683044433594\n",
            "Epoch = 55,training loss = 6078.98876953125, loss = 5310.755859375, MAE = 63.471656799316406\n",
            "Epoch = 56,training loss = 6097.55859375, loss = 5305.54150390625, MAE = 63.386192321777344\n",
            "Epoch = 57,training loss = 6105.7822265625, loss = 5316.46142578125, MAE = 63.55046463012695\n",
            "Epoch = 58,training loss = 6090.93017578125, loss = 5353.64453125, MAE = 63.93136978149414\n",
            "Epoch = 59,training loss = 6077.16015625, loss = 5411.51513671875, MAE = 64.40737915039062\n",
            "Epoch = 60,training loss = 6083.9423828125, loss = 5448.54248046875, MAE = 64.68392181396484\n",
            "Epoch = 61,training loss = 6095.13330078125, loss = 5431.93603515625, MAE = 64.56224060058594\n",
            "Epoch = 62,training loss = 6089.5244140625, loss = 5381.5771484375, MAE = 64.17711639404297\n",
            "Epoch = 63,training loss = 6077.83837890625, loss = 5338.9765625, MAE = 63.79466247558594\n",
            "Epoch = 64,training loss = 6078.99560546875, loss = 5320.310546875, MAE = 63.597721099853516\n",
            "Epoch = 65,training loss = 6087.35205078125, loss = 5320.8203125, MAE = 63.60284423828125\n",
            "Epoch = 66,training loss = 6087.1591796875, loss = 5338.68896484375, MAE = 63.790122985839844\n",
            "Epoch = 67,training loss = 6079.3447265625, loss = 5372.86181640625, MAE = 64.10382843017578\n",
            "Epoch = 68,training loss = 6077.1123046875, loss = 5406.109375, MAE = 64.3659896850586\n",
            "Epoch = 69,training loss = 6082.45361328125, loss = 5412.94970703125, MAE = 64.41881561279297\n",
            "Epoch = 70,training loss = 6084.0888671875, loss = 5389.4853515625, MAE = 64.23873138427734\n",
            "Epoch = 71,training loss = 6079.03173828125, loss = 5357.146484375, MAE = 63.96490478515625\n",
            "Epoch = 72,training loss = 6076.5419921875, loss = 5335.8740234375, MAE = 63.76472091674805\n",
            "Epoch = 73,training loss = 6079.75, loss = 5330.5546875, MAE = 63.70922088623047\n",
            "Epoch = 74,training loss = 6081.5791015625, loss = 5339.88525390625, MAE = 63.80400466918945\n",
            "Epoch = 75,training loss = 6078.70068359375, loss = 5361.36474609375, MAE = 64.00428771972656\n",
            "Epoch = 76,training loss = 6076.4658203125, loss = 5384.982421875, MAE = 64.20396423339844\n",
            "Epoch = 77,training loss = 6078.31787109375, loss = 5394.6259765625, MAE = 64.27787780761719\n",
            "Epoch = 78,training loss = 6079.9423828125, loss = 5383.7626953125, MAE = 64.19438171386719\n",
            "Epoch = 79,training loss = 6078.1435546875, loss = 5362.61669921875, MAE = 64.01577758789062\n",
            "Epoch = 80,training loss = 6076.46533203125, loss = 5345.6552734375, MAE = 63.857513427734375\n",
            "Epoch = 81,training loss = 6077.5771484375, loss = 5339.783203125, MAE = 63.80329513549805\n",
            "Epoch = 82,training loss = 6078.68896484375, loss = 5345.51953125, MAE = 63.85651397705078\n",
            "Epoch = 83,training loss = 6077.568359375, loss = 5360.1328125, MAE = 63.99339294433594\n",
            "Epoch = 84,training loss = 6076.421875, loss = 5376.013671875, MAE = 64.13223266601562\n",
            "Epoch = 85,training loss = 6077.16796875, loss = 5382.68505859375, MAE = 64.18628692626953\n",
            "Epoch = 86,training loss = 6077.947265625, loss = 5375.91943359375, MAE = 64.13142395019531\n",
            "Epoch = 87,training loss = 6077.16259765625, loss = 5361.92626953125, MAE = 64.00975799560547\n",
            "Epoch = 88,training loss = 6076.43212890625, loss = 5350.31787109375, MAE = 63.899967193603516\n",
            "Epoch = 89,training loss = 6076.98193359375, loss = 5346.58984375, MAE = 63.86598205566406\n",
            "Epoch = 90,training loss = 6077.4345703125, loss = 5351.46240234375, MAE = 63.91041946411133\n",
            "Epoch = 91,training loss = 6076.85107421875, loss = 5362.11767578125, MAE = 64.01175689697266\n",
            "Epoch = 92,training loss = 6076.404296875, loss = 5372.28076171875, MAE = 64.10118865966797\n",
            "Epoch = 93,training loss = 6076.8193359375, loss = 5375.20263671875, MAE = 64.12569427490234\n",
            "Epoch = 94,training loss = 6077.06982421875, loss = 5369.37060546875, MAE = 64.076171875\n",
            "Epoch = 95,training loss = 6076.63623046875, loss = 5359.70654296875, MAE = 63.989463806152344\n",
            "Epoch = 96,training loss = 6076.42431640625, loss = 5352.6103515625, MAE = 63.92146682739258\n",
            "Epoch = 97,training loss = 6076.7490234375, loss = 5351.57177734375, MAE = 63.91130828857422\n",
            "Epoch = 98,training loss = 6076.83447265625, loss = 5356.5244140625, MAE = 63.95948028564453\n",
            "Epoch = 99,training loss = 6076.51025390625, loss = 5364.365234375, MAE = 64.03209686279297\n",
            "Epoch = 100,training loss = 6076.43505859375, loss = 5370.05322265625, MAE = 64.08212280273438\n",
            "Epoch = 101,training loss = 6076.669921875, loss = 5369.84716796875, MAE = 64.08036041259766\n",
            "Epoch = 102,training loss = 6076.65576171875, loss = 5364.41748046875, MAE = 64.03260803222656\n",
            "Epoch = 103,training loss = 6076.431640625, loss = 5357.95458984375, MAE = 63.973140716552734\n",
            "Epoch = 104,training loss = 6076.45166015625, loss = 5354.52880859375, MAE = 63.94028854370117\n",
            "Epoch = 105,training loss = 6076.6064453125, loss = 5355.755859375, MAE = 63.9521484375\n",
            "Epoch = 106,training loss = 6076.541015625, loss = 5360.53369140625, MAE = 63.99720764160156\n",
            "Epoch = 107,training loss = 6076.40673828125, loss = 5365.68359375, MAE = 64.04389190673828\n",
            "Epoch = 108,training loss = 6076.4677734375, loss = 5367.7431640625, MAE = 64.06207275390625\n",
            "Epoch = 109,training loss = 6076.54541015625, loss = 5365.5341796875, MAE = 64.04259490966797\n",
            "Epoch = 110,training loss = 6076.4609375, loss = 5361.005859375, MAE = 64.00160217285156\n",
            "Epoch = 111,training loss = 6076.39990234375, loss = 5357.38916015625, MAE = 63.96778869628906\n",
            "Epoch = 112,training loss = 6076.47021484375, loss = 5356.8291015625, MAE = 63.96246337890625\n",
            "Epoch = 113,training loss = 6076.490234375, loss = 5359.37548828125, MAE = 63.986480712890625\n",
            "Epoch = 114,training loss = 6076.41796875, loss = 5363.173828125, MAE = 64.02140045166016\n",
            "Epoch = 115,training loss = 6076.40966796875, loss = 5365.59228515625, MAE = 64.04309844970703\n",
            "Epoch = 116,training loss = 6076.462890625, loss = 5365.05029296875, MAE = 64.03826904296875\n",
            "Epoch = 117,training loss = 6076.447265625, loss = 5362.2041015625, MAE = 64.01258850097656\n",
            "Epoch = 118,training loss = 6076.4013671875, loss = 5359.2431640625, MAE = 63.985252380371094\n",
            "Epoch = 119,training loss = 6076.41943359375, loss = 5358.13037109375, MAE = 63.974815368652344\n",
            "Epoch = 120,training loss = 6076.44580078125, loss = 5359.404296875, MAE = 63.986751556396484\n",
            "Epoch = 121,training loss = 6076.41650390625, loss = 5362.0234375, MAE = 64.01094055175781\n",
            "Epoch = 122,training loss = 6076.39892578125, loss = 5364.10009765625, MAE = 64.0297622680664\n",
            "Epoch = 123,training loss = 6076.423828125, loss = 5364.2119140625, MAE = 64.03077697753906\n",
            "Epoch = 124,training loss = 6076.42626953125, loss = 5362.45849609375, MAE = 64.0149154663086\n",
            "Epoch = 125,training loss = 6076.40185546875, loss = 5360.26123046875, MAE = 63.994728088378906\n",
            "Epoch = 126,training loss = 6076.40478515625, loss = 5359.17626953125, MAE = 63.9846305847168\n",
            "Epoch = 127,training loss = 6076.4208984375, loss = 5359.82763671875, MAE = 63.99070739746094\n",
            "Epoch = 128,training loss = 6076.4091796875, loss = 5361.6162109375, MAE = 64.0072250366211\n",
            "Epoch = 129,training loss = 6076.39794921875, loss = 5363.197265625, MAE = 64.02162170410156\n",
            "Epoch = 130,training loss = 6076.4091796875, loss = 5363.45263671875, MAE = 64.02392578125\n",
            "Epoch = 131,training loss = 6076.41259765625, loss = 5362.3173828125, MAE = 64.01362609863281\n",
            "Epoch = 132,training loss = 6076.40087890625, loss = 5360.74853515625, MAE = 63.999237060546875\n",
            "Epoch = 133,training loss = 6076.4013671875, loss = 5359.904296875, MAE = 63.99142074584961\n",
            "Epoch = 134,training loss = 6076.40869140625, loss = 5360.30712890625, MAE = 63.995155334472656\n",
            "Epoch = 135,training loss = 6076.404296875, loss = 5361.55615234375, MAE = 64.00666809082031\n",
            "Epoch = 136,training loss = 6076.39794921875, loss = 5362.67919921875, MAE = 64.01692199707031\n",
            "Epoch = 137,training loss = 6076.4033203125, loss = 5362.86083984375, MAE = 64.01856994628906\n",
            "Epoch = 138,training loss = 6076.4052734375, loss = 5362.05078125, MAE = 64.0112075805664\n",
            "Epoch = 139,training loss = 6076.39892578125, loss = 5360.94775390625, MAE = 64.0010757446289\n",
            "Epoch = 140,training loss = 6076.39990234375, loss = 5360.3837890625, MAE = 63.99586868286133\n",
            "Epoch = 141,training loss = 6076.4033203125, loss = 5360.72021484375, MAE = 63.99897766113281\n",
            "Epoch = 142,training loss = 6076.4013671875, loss = 5361.62744140625, MAE = 64.00732421875\n",
            "Epoch = 143,training loss = 6076.3984375, loss = 5362.38330078125, MAE = 64.01422882080078\n",
            "Epoch = 144,training loss = 6076.40087890625, loss = 5362.431640625, MAE = 64.01467895507812\n",
            "Epoch = 145,training loss = 6076.4013671875, loss = 5361.802734375, MAE = 64.0089340209961\n",
            "Epoch = 146,training loss = 6076.39794921875, loss = 5361.03564453125, MAE = 64.00189208984375\n",
            "Epoch = 147,training loss = 6076.3994140625, loss = 5360.7197265625, MAE = 63.99897384643555\n",
            "Epoch = 148,training loss = 6076.40087890625, loss = 5361.048828125, MAE = 64.00200653076172\n",
            "Epoch = 149,training loss = 6076.39892578125, loss = 5361.7158203125, MAE = 64.00813293457031\n",
            "Epoch = 150,training loss = 6076.39794921875, loss = 5362.1826171875, MAE = 64.01239776611328\n",
            "Epoch = 151,training loss = 6076.39892578125, loss = 5362.107421875, MAE = 64.01171112060547\n",
            "Epoch = 152,training loss = 6076.39892578125, loss = 5361.599609375, MAE = 64.00706481933594\n",
            "Epoch = 153,training loss = 6076.39794921875, loss = 5361.09228515625, MAE = 64.00241088867188\n",
            "Epoch = 154,training loss = 6076.3984375, loss = 5360.9755859375, MAE = 64.00133514404297\n",
            "Epoch = 155,training loss = 6076.39892578125, loss = 5361.3017578125, MAE = 64.00433349609375\n",
            "Epoch = 156,training loss = 6076.3984375, loss = 5361.77734375, MAE = 64.0086898803711\n",
            "Epoch = 157,training loss = 6076.3984375, loss = 5362.01611328125, MAE = 64.01087951660156\n",
            "Epoch = 158,training loss = 6076.39892578125, loss = 5361.8525390625, MAE = 64.00938415527344\n",
            "Epoch = 159,training loss = 6076.39794921875, loss = 5361.45361328125, MAE = 64.00572967529297\n",
            "Epoch = 160,training loss = 6076.39794921875, loss = 5361.1572265625, MAE = 64.00300598144531\n",
            "Epoch = 161,training loss = 6076.3984375, loss = 5361.1875, MAE = 64.00328826904297\n",
            "Epoch = 162,training loss = 6076.3984375, loss = 5361.49072265625, MAE = 64.00606536865234\n",
            "Epoch = 163,training loss = 6076.39794921875, loss = 5361.798828125, MAE = 64.00889587402344\n",
            "Epoch = 164,training loss = 6076.3974609375, loss = 5361.8662109375, MAE = 64.00950622558594\n",
            "Epoch = 165,training loss = 6076.39794921875, loss = 5361.6591796875, MAE = 64.00761413574219\n",
            "Epoch = 166,training loss = 6076.39794921875, loss = 5361.37255859375, MAE = 64.00497436523438\n",
            "Epoch = 167,training loss = 6076.39794921875, loss = 5361.2431640625, MAE = 64.00379943847656\n",
            "Epoch = 168,training loss = 6076.39794921875, loss = 5361.36181640625, MAE = 64.0048828125\n",
            "Epoch = 169,training loss = 6076.3984375, loss = 5361.6083984375, MAE = 64.00714874267578\n",
            "Epoch = 170,training loss = 6076.3984375, loss = 5361.77099609375, MAE = 64.00863647460938\n",
            "Epoch = 171,training loss = 6076.3974609375, loss = 5361.72265625, MAE = 64.00819396972656\n",
            "Epoch = 172,training loss = 6076.39794921875, loss = 5361.52490234375, MAE = 64.00637817382812\n",
            "Epoch = 173,training loss = 6076.39794921875, loss = 5361.3525390625, MAE = 64.00479888916016\n",
            "Epoch = 174,training loss = 6076.3984375, loss = 5361.3466796875, MAE = 64.00474548339844\n",
            "Epoch = 175,training loss = 6076.39794921875, loss = 5361.49560546875, MAE = 64.00611114501953\n",
            "Epoch = 176,training loss = 6076.39794921875, loss = 5361.6611328125, MAE = 64.00762939453125\n",
            "Epoch = 177,training loss = 6076.39794921875, loss = 5361.70458984375, MAE = 64.00802612304688\n",
            "Epoch = 178,training loss = 6076.39794921875, loss = 5361.60009765625, MAE = 64.00707244873047\n",
            "Epoch = 179,training loss = 6076.39794921875, loss = 5361.451171875, MAE = 64.00570678710938\n",
            "Epoch = 180,training loss = 6076.39794921875, loss = 5361.38525390625, MAE = 64.0051040649414\n",
            "Epoch = 181,training loss = 6076.39794921875, loss = 5361.45166015625, MAE = 64.00570678710938\n",
            "Epoch = 182,training loss = 6076.3984375, loss = 5361.58056640625, MAE = 64.00689697265625\n",
            "Epoch = 183,training loss = 6076.3984375, loss = 5361.6572265625, MAE = 64.00759887695312\n",
            "Epoch = 184,training loss = 6076.39794921875, loss = 5361.62109375, MAE = 64.00726318359375\n",
            "Epoch = 185,training loss = 6076.39794921875, loss = 5361.513671875, MAE = 64.00627899169922\n",
            "Epoch = 186,training loss = 6076.39794921875, loss = 5361.43359375, MAE = 64.00554656982422\n",
            "Epoch = 187,training loss = 6076.39794921875, loss = 5361.44677734375, MAE = 64.00566864013672\n",
            "Epoch = 188,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00645446777344\n",
            "Epoch = 189,training loss = 6076.3974609375, loss = 5361.61181640625, MAE = 64.0071792602539\n",
            "Epoch = 190,training loss = 6076.39794921875, loss = 5361.6142578125, MAE = 64.0072021484375\n",
            "Epoch = 191,training loss = 6076.39794921875, loss = 5361.54736328125, MAE = 64.00658416748047\n",
            "Epoch = 192,training loss = 6076.39794921875, loss = 5361.4736328125, MAE = 64.00591278076172\n",
            "Epoch = 193,training loss = 6076.39794921875, loss = 5361.46044921875, MAE = 64.00579071044922\n",
            "Epoch = 194,training loss = 6076.39794921875, loss = 5361.51171875, MAE = 64.00625610351562\n",
            "Epoch = 195,training loss = 6076.39794921875, loss = 5361.57666015625, MAE = 64.00686645507812\n",
            "Epoch = 196,training loss = 6076.39794921875, loss = 5361.59814453125, MAE = 64.00704956054688\n",
            "Epoch = 197,training loss = 6076.39794921875, loss = 5361.5595703125, MAE = 64.00669860839844\n",
            "Epoch = 198,training loss = 6076.39794921875, loss = 5361.50146484375, MAE = 64.00617218017578\n",
            "Epoch = 199,training loss = 6076.39794921875, loss = 5361.47802734375, MAE = 64.00595092773438\n",
            "Epoch = 200,training loss = 6076.3984375, loss = 5361.505859375, MAE = 64.00621032714844\n",
            "Epoch = 201,training loss = 6076.39794921875, loss = 5361.5556640625, MAE = 64.00666046142578\n",
            "Epoch = 202,training loss = 6076.39794921875, loss = 5361.58056640625, MAE = 64.00689697265625\n",
            "Epoch = 203,training loss = 6076.3974609375, loss = 5361.56103515625, MAE = 64.0067138671875\n",
            "Epoch = 204,training loss = 6076.39794921875, loss = 5361.51806640625, MAE = 64.0063247680664\n",
            "Epoch = 205,training loss = 6076.3974609375, loss = 5361.4931640625, MAE = 64.00608825683594\n",
            "Epoch = 206,training loss = 6076.3974609375, loss = 5361.50732421875, MAE = 64.00621795654297\n",
            "Epoch = 207,training loss = 6076.39794921875, loss = 5361.54345703125, MAE = 64.00655364990234\n",
            "Epoch = 208,training loss = 6076.39794921875, loss = 5361.56689453125, MAE = 64.00676727294922\n",
            "Epoch = 209,training loss = 6076.39794921875, loss = 5361.5576171875, MAE = 64.00668334960938\n",
            "Epoch = 210,training loss = 6076.39794921875, loss = 5361.52734375, MAE = 64.00640106201172\n",
            "Epoch = 211,training loss = 6076.39794921875, loss = 5361.505859375, MAE = 64.00621032714844\n",
            "Epoch = 212,training loss = 6076.3984375, loss = 5361.51171875, MAE = 64.00626373291016\n",
            "Epoch = 213,training loss = 6076.39794921875, loss = 5361.537109375, MAE = 64.00650024414062\n",
            "Epoch = 214,training loss = 6076.39794921875, loss = 5361.556640625, MAE = 64.00667572021484\n",
            "Epoch = 215,training loss = 6076.39794921875, loss = 5361.552734375, MAE = 64.00664520263672\n",
            "Epoch = 216,training loss = 6076.39794921875, loss = 5361.53125, MAE = 64.00643920898438\n",
            "Epoch = 217,training loss = 6076.39794921875, loss = 5361.51416015625, MAE = 64.00627899169922\n",
            "Epoch = 218,training loss = 6076.39794921875, loss = 5361.51611328125, MAE = 64.00630950927734\n",
            "Epoch = 219,training loss = 6076.39794921875, loss = 5361.53466796875, MAE = 64.0064697265625\n",
            "Epoch = 220,training loss = 6076.39794921875, loss = 5361.5498046875, MAE = 64.0066146850586\n",
            "Epoch = 221,training loss = 6076.39794921875, loss = 5361.54833984375, MAE = 64.00659942626953\n",
            "Epoch = 222,training loss = 6076.39794921875, loss = 5361.5322265625, MAE = 64.00645446777344\n",
            "Epoch = 223,training loss = 6076.39794921875, loss = 5361.51904296875, MAE = 64.00633239746094\n",
            "Epoch = 224,training loss = 6076.39794921875, loss = 5361.52001953125, MAE = 64.00634002685547\n",
            "Epoch = 225,training loss = 6076.39794921875, loss = 5361.53466796875, MAE = 64.0064697265625\n",
            "Epoch = 226,training loss = 6076.3984375, loss = 5361.5458984375, MAE = 64.0065689086914\n",
            "Epoch = 227,training loss = 6076.39794921875, loss = 5361.54443359375, MAE = 64.00656127929688\n",
            "Epoch = 228,training loss = 6076.39794921875, loss = 5361.5322265625, MAE = 64.00645446777344\n",
            "Epoch = 229,training loss = 6076.39794921875, loss = 5361.52294921875, MAE = 64.00636291503906\n",
            "Epoch = 230,training loss = 6076.39794921875, loss = 5361.52392578125, MAE = 64.0063705444336\n",
            "Epoch = 231,training loss = 6076.39794921875, loss = 5361.53466796875, MAE = 64.00646209716797\n",
            "Epoch = 232,training loss = 6076.39794921875, loss = 5361.54296875, MAE = 64.00654602050781\n",
            "Epoch = 233,training loss = 6076.39794921875, loss = 5361.54150390625, MAE = 64.00653839111328\n",
            "Epoch = 234,training loss = 6076.39794921875, loss = 5361.53173828125, MAE = 64.00645446777344\n",
            "Epoch = 235,training loss = 6076.3974609375, loss = 5361.525390625, MAE = 64.00639343261719\n",
            "Epoch = 236,training loss = 6076.39794921875, loss = 5361.52685546875, MAE = 64.00640106201172\n",
            "Epoch = 237,training loss = 6076.39794921875, loss = 5361.53466796875, MAE = 64.0064697265625\n",
            "Epoch = 238,training loss = 6076.39794921875, loss = 5361.54052734375, MAE = 64.00653076171875\n",
            "Epoch = 239,training loss = 6076.39794921875, loss = 5361.53955078125, MAE = 64.00650787353516\n",
            "Epoch = 240,training loss = 6076.39794921875, loss = 5361.53173828125, MAE = 64.00643920898438\n",
            "Epoch = 241,training loss = 6076.39794921875, loss = 5361.52734375, MAE = 64.00640106201172\n",
            "Epoch = 242,training loss = 6076.39794921875, loss = 5361.52880859375, MAE = 64.00642395019531\n",
            "Epoch = 243,training loss = 6076.39794921875, loss = 5361.53515625, MAE = 64.0064697265625\n",
            "Epoch = 244,training loss = 6076.39794921875, loss = 5361.53857421875, MAE = 64.00650787353516\n",
            "Epoch = 245,training loss = 6076.39794921875, loss = 5361.53662109375, MAE = 64.0064926147461\n",
            "Epoch = 246,training loss = 6076.39794921875, loss = 5361.53173828125, MAE = 64.0064468383789\n",
            "Epoch = 247,training loss = 6076.39794921875, loss = 5361.5283203125, MAE = 64.00642395019531\n",
            "Epoch = 248,training loss = 6076.39794921875, loss = 5361.53076171875, MAE = 64.00643157958984\n",
            "Epoch = 249,training loss = 6076.39794921875, loss = 5361.53564453125, MAE = 64.00647735595703\n",
            "Epoch = 250,training loss = 6076.39794921875, loss = 5361.53759765625, MAE = 64.00650024414062\n",
            "Epoch = 251,training loss = 6076.39794921875, loss = 5361.53564453125, MAE = 64.00647735595703\n",
            "Epoch = 252,training loss = 6076.39794921875, loss = 5361.53125, MAE = 64.00643920898438\n",
            "Epoch = 253,training loss = 6076.3984375, loss = 5361.5302734375, MAE = 64.00643157958984\n",
            "Epoch = 254,training loss = 6076.39794921875, loss = 5361.5322265625, MAE = 64.0064468383789\n",
            "Epoch = 255,training loss = 6076.39794921875, loss = 5361.53564453125, MAE = 64.00647735595703\n",
            "Epoch = 256,training loss = 6076.39794921875, loss = 5361.5361328125, MAE = 64.0064926147461\n",
            "Epoch = 257,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.0064697265625\n",
            "Epoch = 258,training loss = 6076.39794921875, loss = 5361.53173828125, MAE = 64.00643920898438\n",
            "Epoch = 259,training loss = 6076.39794921875, loss = 5361.53125, MAE = 64.00643920898438\n",
            "Epoch = 260,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 261,training loss = 6076.39794921875, loss = 5361.53564453125, MAE = 64.00647735595703\n",
            "Epoch = 262,training loss = 6076.3984375, loss = 5361.53564453125, MAE = 64.00647735595703\n",
            "Epoch = 263,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 264,training loss = 6076.39794921875, loss = 5361.53173828125, MAE = 64.00643920898438\n",
            "Epoch = 265,training loss = 6076.39794921875, loss = 5361.53173828125, MAE = 64.0064468383789\n",
            "Epoch = 266,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 267,training loss = 6076.39794921875, loss = 5361.53515625, MAE = 64.00647735595703\n",
            "Epoch = 268,training loss = 6076.39794921875, loss = 5361.53466796875, MAE = 64.0064697265625\n",
            "Epoch = 269,training loss = 6076.3984375, loss = 5361.5322265625, MAE = 64.00645446777344\n",
            "Epoch = 270,training loss = 6076.39794921875, loss = 5361.53173828125, MAE = 64.0064468383789\n",
            "Epoch = 271,training loss = 6076.39794921875, loss = 5361.5322265625, MAE = 64.00645446777344\n",
            "Epoch = 272,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.0064697265625\n",
            "Epoch = 273,training loss = 6076.39794921875, loss = 5361.53466796875, MAE = 64.0064697265625\n",
            "Epoch = 274,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 275,training loss = 6076.39794921875, loss = 5361.5322265625, MAE = 64.00645446777344\n",
            "Epoch = 276,training loss = 6076.39794921875, loss = 5361.5322265625, MAE = 64.00645446777344\n",
            "Epoch = 277,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 278,training loss = 6076.39794921875, loss = 5361.53466796875, MAE = 64.00646209716797\n",
            "Epoch = 279,training loss = 6076.3984375, loss = 5361.53466796875, MAE = 64.0064697265625\n",
            "Epoch = 280,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 281,training loss = 6076.39794921875, loss = 5361.5322265625, MAE = 64.00645446777344\n",
            "Epoch = 282,training loss = 6076.3984375, loss = 5361.5322265625, MAE = 64.00645446777344\n",
            "Epoch = 283,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 284,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 285,training loss = 6076.39794921875, loss = 5361.53466796875, MAE = 64.00646209716797\n",
            "Epoch = 286,training loss = 6076.39794921875, loss = 5361.5322265625, MAE = 64.00646209716797\n",
            "Epoch = 287,training loss = 6076.3984375, loss = 5361.5322265625, MAE = 64.00646209716797\n",
            "Epoch = 288,training loss = 6076.3984375, loss = 5361.5322265625, MAE = 64.00646209716797\n",
            "Epoch = 289,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 290,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.0064697265625\n",
            "Epoch = 291,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 292,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 293,training loss = 6076.3984375, loss = 5361.5322265625, MAE = 64.00646209716797\n",
            "Epoch = 294,training loss = 6076.3984375, loss = 5361.5322265625, MAE = 64.00646209716797\n",
            "Epoch = 295,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 296,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 297,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 298,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 299,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 300,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00645446777344\n",
            "Epoch = 301,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 302,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 303,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 304,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 305,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 306,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 307,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 308,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 309,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 310,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 311,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 312,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 313,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 314,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 315,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 316,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 317,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 318,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 319,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 320,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 321,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 322,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 323,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 324,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 325,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 326,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 327,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 328,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 329,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 330,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 331,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 332,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 333,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 334,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 335,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 336,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 337,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 338,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 339,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 340,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 341,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 342,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 343,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 344,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 345,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 346,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 347,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 348,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 349,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 350,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 351,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 352,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 353,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 354,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 355,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 356,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 357,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 358,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 359,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 360,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 361,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 362,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 363,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 364,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 365,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 366,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 367,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 368,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 369,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 370,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 371,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 372,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 373,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 374,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 375,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 376,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 377,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 378,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 379,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 380,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 381,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 382,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 383,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 384,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 385,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 386,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 387,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 388,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 389,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 390,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 391,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 392,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 393,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 394,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 395,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 396,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 397,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 398,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 399,training loss = 6076.39794921875, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 400,training loss = 6076.39794921875, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 401,training loss = 6076.39794921875, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 402,training loss = 6076.39794921875, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 403,training loss = 6076.39794921875, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 404,training loss = 6076.39794921875, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 405,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 406,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 407,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 408,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 409,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 410,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 411,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 412,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 413,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 414,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 415,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 416,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 417,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 418,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 419,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 420,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 421,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 422,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 423,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 424,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 425,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 426,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 427,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 428,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 429,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 430,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 431,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 432,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 433,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 434,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 435,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 436,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 437,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 438,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 439,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 440,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 441,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 442,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 443,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 444,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 445,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 446,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 447,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 448,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 449,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 450,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 451,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 452,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 453,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 454,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 455,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 456,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 457,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 458,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 459,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 460,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 461,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 462,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 463,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 464,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 465,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 466,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 467,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 468,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 469,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 470,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 471,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 472,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 473,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 474,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 475,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 476,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 477,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 478,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 479,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 480,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 481,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 482,training loss = 6076.39794921875, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 483,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 484,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 485,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 486,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 487,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 488,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 489,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 490,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 491,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 492,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 493,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 494,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 495,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 496,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 497,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 498,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 499,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 500,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 501,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 502,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 503,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 504,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 505,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 506,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 507,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 508,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 509,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 510,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 511,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 512,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 513,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 514,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 515,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 516,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 517,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 518,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 519,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 520,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 521,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 522,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 523,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 524,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 525,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 526,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 527,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 528,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 529,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 530,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 531,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 532,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 533,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 534,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 535,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 536,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 537,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 538,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 539,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 540,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 541,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 542,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 543,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 544,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 545,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 546,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 547,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 548,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 549,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 550,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 551,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 552,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 553,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 554,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 555,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 556,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 557,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 558,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 559,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 560,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 561,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 562,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 563,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 564,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 565,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 566,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 567,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 568,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 569,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 570,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 571,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 572,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 573,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 574,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 575,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 576,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 577,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 578,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 579,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 580,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 581,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 582,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 583,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 584,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 585,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 586,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 587,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 588,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 589,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 590,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 591,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 592,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 593,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 594,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 595,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 596,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 597,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 598,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 599,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 600,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 601,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 602,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 603,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 604,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 605,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 606,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 607,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 608,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 609,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 610,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 611,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 612,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 613,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 614,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 615,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 616,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 617,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 618,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 619,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 620,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 621,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 622,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 623,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 624,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 625,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 626,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 627,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 628,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 629,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 630,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 631,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 632,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 633,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 634,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 635,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 636,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 637,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 638,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 639,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 640,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 641,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 642,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 643,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 644,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 645,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 646,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 647,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 648,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 649,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 650,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 651,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 652,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 653,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 654,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 655,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 656,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 657,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 658,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 659,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 660,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 661,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 662,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 663,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 664,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 665,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 666,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 667,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 668,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 669,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 670,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 671,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 672,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 673,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 674,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 675,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 676,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 677,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 678,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 679,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 680,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 681,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 682,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 683,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 684,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 685,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 686,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 687,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 688,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 689,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 690,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 691,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 692,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 693,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 694,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 695,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 696,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 697,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 698,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 699,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 700,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 701,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 702,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 703,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 704,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 705,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 706,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 707,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 708,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 709,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 710,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 711,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 712,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 713,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 714,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 715,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 716,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 717,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 718,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 719,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 720,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 721,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 722,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 723,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 724,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 725,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 726,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 727,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 728,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 729,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 730,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 731,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 732,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 733,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 734,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 735,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 736,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 737,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 738,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 739,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 740,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 741,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 742,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 743,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 744,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 745,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 746,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 747,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 748,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 749,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 750,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 751,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 752,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 753,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 754,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 755,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 756,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 757,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 758,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 759,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 760,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 761,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 762,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 763,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 764,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 765,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 766,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 767,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 768,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 769,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 770,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 771,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 772,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 773,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 774,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 775,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 776,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 777,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 778,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 779,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 780,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 781,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 782,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 783,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 784,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 785,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 786,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 787,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 788,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 789,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 790,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 791,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 792,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 793,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 794,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 795,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 796,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 797,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 798,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 799,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 800,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 801,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 802,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 803,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 804,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 805,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 806,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 807,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 808,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 809,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 810,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 811,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 812,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 813,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 814,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 815,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 816,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 817,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 818,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 819,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 820,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 821,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 822,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 823,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 824,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 825,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 826,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 827,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 828,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 829,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 830,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 831,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 832,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 833,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 834,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 835,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 836,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 837,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 838,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 839,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 840,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 841,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 842,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 843,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 844,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 845,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 846,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 847,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 848,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 849,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 850,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 851,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 852,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 853,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 854,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 855,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 856,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 857,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 858,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 859,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 860,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 861,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 862,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 863,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 864,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 865,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 866,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 867,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 868,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 869,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 870,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 871,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 872,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 873,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 874,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 875,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 876,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 877,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 878,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 879,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 880,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 881,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 882,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 883,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 884,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 885,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 886,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 887,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 888,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 889,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 890,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 891,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 892,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 893,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 894,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 895,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 896,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 897,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 898,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 899,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 900,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 901,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 902,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 903,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 904,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 905,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 906,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 907,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 908,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 909,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 910,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 911,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 912,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 913,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 914,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 915,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 916,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 917,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 918,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 919,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 920,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 921,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 922,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 923,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 924,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 925,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 926,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 927,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 928,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 929,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 930,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 931,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 932,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 933,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 934,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 935,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 936,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 937,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 938,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 939,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 940,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 941,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 942,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 943,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 944,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 945,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 946,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 947,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 948,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 949,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 950,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 951,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 952,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 953,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 954,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 955,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 956,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 957,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 958,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 959,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 960,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 961,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 962,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 963,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 964,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 965,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 966,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 967,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 968,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 969,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 970,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 971,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 972,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 973,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 974,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 975,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 976,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 977,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 978,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 979,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 980,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 981,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 982,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 983,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 984,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 985,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 986,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 987,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 988,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 989,training loss = 6076.3984375, loss = 5361.533203125, MAE = 64.00646209716797\n",
            "Epoch = 990,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 991,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 992,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 993,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 994,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 995,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 996,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 997,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 998,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n",
            "Epoch = 999,training loss = 6076.3984375, loss = 5361.5341796875, MAE = 64.00646209716797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(y_hat), type(X_train), type(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRErgjN2QOmi",
        "outputId": "8a540bb8-d112-45bb-ed42-9ba5dba32209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Tensor, torch.Tensor, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOBI5r_pQTZZ",
        "outputId": "e961eafd-fbc9-4796-c9c7-3486f72bf7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(442, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXd0pyJHRYjQ",
        "outputId": "d5ba9cb8-564e-429f-91dc-e990ca63f7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "       220.,  57.])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VH-V0yAtRjjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y0ku75KB2Og7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-nGXS1gj2Ojx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION **2** **bold text**"
      ],
      "metadata": {
        "id": "_3G0oBOw2Oxb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8hcP8Hm2Ttr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Pakf5J512Twt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "PW48RGHH2W7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNsJNITg20iB",
        "outputId": "b2e714eb-eb97-40f5-e9f9-01b7b27416b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['malignant', 'benign'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RuNLFk22778",
        "outputId": "3fec7188-5c37-4dc0-e0eb-177983202425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "        1.189e-01],\n",
              "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "        8.902e-02],\n",
              "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "        8.758e-02],\n",
              "       ...,\n",
              "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "        7.820e-02],\n",
              "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "        1.240e-01],\n",
              "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "        7.039e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.data\n",
        "y = data.target"
      ],
      "metadata": {
        "id": "yr6SsUpR3Tfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlPB_6M33qCw",
        "outputId": "da92c71b-b5c2-4482-99ac-4cf77f9e35ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "s_X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "U3vlG4GK3sPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr3PngXv5TnE",
        "outputId": "ae3c60ab-c000-41ba-92d2-c9b7bcc0908e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.52103744, 0.0226581 , 0.54598853, ..., 0.91202749, 0.59846245,\n",
              "        0.41886396],\n",
              "       [0.64314449, 0.27257355, 0.61578329, ..., 0.63917526, 0.23358959,\n",
              "        0.22287813],\n",
              "       [0.60149557, 0.3902604 , 0.59574321, ..., 0.83505155, 0.40370589,\n",
              "        0.21343303],\n",
              "       ...,\n",
              "       [0.45525108, 0.62123774, 0.44578813, ..., 0.48728522, 0.12872068,\n",
              "        0.1519087 ],\n",
              "       [0.64456434, 0.66351031, 0.66553797, ..., 0.91065292, 0.49714173,\n",
              "        0.45231536],\n",
              "       [0.03686876, 0.50152181, 0.02853984, ..., 0.        , 0.25744136,\n",
              "        0.10068215]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(s_X, y,test_size = 0.2,random_state=42)\n",
        "X_train_new = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test_new = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train_new = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test_new = torch.from_numpy(y_test.astype(np.float32))"
      ],
      "metadata": {
        "id": "pTU_wPSX88xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3G4_f0BHsuA",
        "outputId": "446f8a47-8abd-40e9-99ea-301e4a07a343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([455, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class bc_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_one = nn.Linear(s_X.shape[1],16)\n",
        "    self.act1 = nn.ReLU6()\n",
        "    self.layer_two = nn.Linear(16,32)\n",
        "    self.layer_three = nn.Linear(32,1)\n",
        "    self.act2 = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.layer_one(x)\n",
        "    x = self.act1(x)\n",
        "    x = self.layer_two(x)\n",
        "    x = self.act1(x)\n",
        "    x = self.layer_three(x)\n",
        "    x = self.act2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "SjyLj_r75Vof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod = bc_model()\n",
        "error_function = nn.BCELoss()\n",
        "optimizer1 = torch.optim.Adam(params=mod.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "1NDN0_J774QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 300\n",
        "for epoch in range(epochs):\n",
        "  mod.train()\n",
        "  y_hat = mod(X_train_new)\n",
        "  los = error_function(y_hat,y_train_new.unsqueeze(dim=1))\n",
        "  optimizer1.zero_grad()\n",
        "  los.backward()\n",
        "  optimizer1.step()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    y_pred = mod(X_test_new)\n",
        "    test_loss = error_function(y_pred,y_test_new.unsqueeze(dim=1))\n",
        "    print(f'Epoch = {epoch},.....trainning_loss={los}.....test_loss ={test_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvMM_F3d7uXw",
        "outputId": "6c073e49-64b3-4a8f-a477-1a6f54531f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch = 0,.....trainning_loss=0.6824575662612915.....test_loss =0.6811676025390625\n",
            "Epoch = 1,.....trainning_loss=0.6809895038604736.....test_loss =0.6796764135360718\n",
            "Epoch = 2,.....trainning_loss=0.6795321106910706.....test_loss =0.6781827211380005\n",
            "Epoch = 3,.....trainning_loss=0.6780824065208435.....test_loss =0.6766963601112366\n",
            "Epoch = 4,.....trainning_loss=0.67664635181427.....test_loss =0.675230860710144\n",
            "Epoch = 5,.....trainning_loss=0.6752135157585144.....test_loss =0.6737440824508667\n",
            "Epoch = 6,.....trainning_loss=0.6737599968910217.....test_loss =0.6722123622894287\n",
            "Epoch = 7,.....trainning_loss=0.6722897291183472.....test_loss =0.6706504821777344\n",
            "Epoch = 8,.....trainning_loss=0.6708001494407654.....test_loss =0.6690391302108765\n",
            "Epoch = 9,.....trainning_loss=0.6692839860916138.....test_loss =0.6673839092254639\n",
            "Epoch = 10,.....trainning_loss=0.6677375435829163.....test_loss =0.6657004952430725\n",
            "Epoch = 11,.....trainning_loss=0.6661493182182312.....test_loss =0.6639774441719055\n",
            "Epoch = 12,.....trainning_loss=0.6645215153694153.....test_loss =0.6622145175933838\n",
            "Epoch = 13,.....trainning_loss=0.6628568768501282.....test_loss =0.660424530506134\n",
            "Epoch = 14,.....trainning_loss=0.6611546874046326.....test_loss =0.6586143970489502\n",
            "Epoch = 15,.....trainning_loss=0.6594225764274597.....test_loss =0.6567643284797668\n",
            "Epoch = 16,.....trainning_loss=0.6576681137084961.....test_loss =0.6548653841018677\n",
            "Epoch = 17,.....trainning_loss=0.6558815836906433.....test_loss =0.6529291272163391\n",
            "Epoch = 18,.....trainning_loss=0.6540750861167908.....test_loss =0.6509543061256409\n",
            "Epoch = 19,.....trainning_loss=0.6522459983825684.....test_loss =0.6489459872245789\n",
            "Epoch = 20,.....trainning_loss=0.650378942489624.....test_loss =0.646902859210968\n",
            "Epoch = 21,.....trainning_loss=0.6484871506690979.....test_loss =0.6448367834091187\n",
            "Epoch = 22,.....trainning_loss=0.6465732455253601.....test_loss =0.6427530646324158\n",
            "Epoch = 23,.....trainning_loss=0.6446325182914734.....test_loss =0.6406390070915222\n",
            "Epoch = 24,.....trainning_loss=0.6426631212234497.....test_loss =0.6384949684143066\n",
            "Epoch = 25,.....trainning_loss=0.640649676322937.....test_loss =0.6363192796707153\n",
            "Epoch = 26,.....trainning_loss=0.6385911107063293.....test_loss =0.6340962648391724\n",
            "Epoch = 27,.....trainning_loss=0.6364862322807312.....test_loss =0.6318303346633911\n",
            "Epoch = 28,.....trainning_loss=0.6343351006507874.....test_loss =0.6295022964477539\n",
            "Epoch = 29,.....trainning_loss=0.6321330666542053.....test_loss =0.6271082162857056\n",
            "Epoch = 30,.....trainning_loss=0.6298753023147583.....test_loss =0.6246339082717896\n",
            "Epoch = 31,.....trainning_loss=0.627559244632721.....test_loss =0.6220827698707581\n",
            "Epoch = 32,.....trainning_loss=0.6251805424690247.....test_loss =0.619462251663208\n",
            "Epoch = 33,.....trainning_loss=0.6227415204048157.....test_loss =0.6167833805084229\n",
            "Epoch = 34,.....trainning_loss=0.6202376484870911.....test_loss =0.6140283346176147\n",
            "Epoch = 35,.....trainning_loss=0.6176678538322449.....test_loss =0.6111882328987122\n",
            "Epoch = 36,.....trainning_loss=0.6150241494178772.....test_loss =0.6082678437232971\n",
            "Epoch = 37,.....trainning_loss=0.6123018264770508.....test_loss =0.6052512526512146\n",
            "Epoch = 38,.....trainning_loss=0.6094921231269836.....test_loss =0.6021345853805542\n",
            "Epoch = 39,.....trainning_loss=0.606582760810852.....test_loss =0.5989179611206055\n",
            "Epoch = 40,.....trainning_loss=0.6035807132720947.....test_loss =0.5955950617790222\n",
            "Epoch = 41,.....trainning_loss=0.6004827618598938.....test_loss =0.5921534299850464\n",
            "Epoch = 42,.....trainning_loss=0.5972845554351807.....test_loss =0.5886167287826538\n",
            "Epoch = 43,.....trainning_loss=0.5939867496490479.....test_loss =0.5849807858467102\n",
            "Epoch = 44,.....trainning_loss=0.5906136631965637.....test_loss =0.5812847018241882\n",
            "Epoch = 45,.....trainning_loss=0.5871706604957581.....test_loss =0.5775032043457031\n",
            "Epoch = 46,.....trainning_loss=0.5836443305015564.....test_loss =0.5736578702926636\n",
            "Epoch = 47,.....trainning_loss=0.5800451636314392.....test_loss =0.56974858045578\n",
            "Epoch = 48,.....trainning_loss=0.5763739347457886.....test_loss =0.565763533115387\n",
            "Epoch = 49,.....trainning_loss=0.5726346969604492.....test_loss =0.5616949796676636\n",
            "Epoch = 50,.....trainning_loss=0.5688074827194214.....test_loss =0.5575506687164307\n",
            "Epoch = 51,.....trainning_loss=0.5648854970932007.....test_loss =0.553310751914978\n",
            "Epoch = 52,.....trainning_loss=0.5608754754066467.....test_loss =0.5489699840545654\n",
            "Epoch = 53,.....trainning_loss=0.5567662715911865.....test_loss =0.5445199608802795\n",
            "Epoch = 54,.....trainning_loss=0.5525612235069275.....test_loss =0.5399635434150696\n",
            "Epoch = 55,.....trainning_loss=0.5482568740844727.....test_loss =0.5353060364723206\n",
            "Epoch = 56,.....trainning_loss=0.5438554286956787.....test_loss =0.5305619835853577\n",
            "Epoch = 57,.....trainning_loss=0.53935706615448.....test_loss =0.525725245475769\n",
            "Epoch = 58,.....trainning_loss=0.5347698926925659.....test_loss =0.5208083987236023\n",
            "Epoch = 59,.....trainning_loss=0.5301012396812439.....test_loss =0.515811026096344\n",
            "Epoch = 60,.....trainning_loss=0.5253610610961914.....test_loss =0.5107502341270447\n",
            "Epoch = 61,.....trainning_loss=0.5205532908439636.....test_loss =0.5056174397468567\n",
            "Epoch = 62,.....trainning_loss=0.5156793594360352.....test_loss =0.5004203915596008\n",
            "Epoch = 63,.....trainning_loss=0.5107442736625671.....test_loss =0.4951629638671875\n",
            "Epoch = 64,.....trainning_loss=0.5057451128959656.....test_loss =0.48984259366989136\n",
            "Epoch = 65,.....trainning_loss=0.5006835460662842.....test_loss =0.48447155952453613\n",
            "Epoch = 66,.....trainning_loss=0.4955660104751587.....test_loss =0.4790523648262024\n",
            "Epoch = 67,.....trainning_loss=0.4903894364833832.....test_loss =0.4735851287841797\n",
            "Epoch = 68,.....trainning_loss=0.4851551055908203.....test_loss =0.4680660665035248\n",
            "Epoch = 69,.....trainning_loss=0.4798661172389984.....test_loss =0.4624936282634735\n",
            "Epoch = 70,.....trainning_loss=0.47452765703201294.....test_loss =0.45688295364379883\n",
            "Epoch = 71,.....trainning_loss=0.46915480494499207.....test_loss =0.4512331783771515\n",
            "Epoch = 72,.....trainning_loss=0.46374815702438354.....test_loss =0.4455582797527313\n",
            "Epoch = 73,.....trainning_loss=0.45831266045570374.....test_loss =0.4398581087589264\n",
            "Epoch = 74,.....trainning_loss=0.4528532326221466.....test_loss =0.4341314136981964\n",
            "Epoch = 75,.....trainning_loss=0.4473738372325897.....test_loss =0.42839315533638\n",
            "Epoch = 76,.....trainning_loss=0.44187983870506287.....test_loss =0.42263904213905334\n",
            "Epoch = 77,.....trainning_loss=0.4363783597946167.....test_loss =0.4168797433376312\n",
            "Epoch = 78,.....trainning_loss=0.4308733642101288.....test_loss =0.41112715005874634\n",
            "Epoch = 79,.....trainning_loss=0.4253685772418976.....test_loss =0.405383437871933\n",
            "Epoch = 80,.....trainning_loss=0.4198731482028961.....test_loss =0.399651437997818\n",
            "Epoch = 81,.....trainning_loss=0.41438978910446167.....test_loss =0.3939499855041504\n",
            "Epoch = 82,.....trainning_loss=0.4089224338531494.....test_loss =0.3882678747177124\n",
            "Epoch = 83,.....trainning_loss=0.4034801423549652.....test_loss =0.3826148509979248\n",
            "Epoch = 84,.....trainning_loss=0.39806032180786133.....test_loss =0.37698352336883545\n",
            "Epoch = 85,.....trainning_loss=0.39265894889831543.....test_loss =0.37138110399246216\n",
            "Epoch = 86,.....trainning_loss=0.38728249073028564.....test_loss =0.36581847071647644\n",
            "Epoch = 87,.....trainning_loss=0.381945937871933.....test_loss =0.36030033230781555\n",
            "Epoch = 88,.....trainning_loss=0.37664875388145447.....test_loss =0.35481134057044983\n",
            "Epoch = 89,.....trainning_loss=0.37138769030570984.....test_loss =0.34937533736228943\n",
            "Epoch = 90,.....trainning_loss=0.3661690056324005.....test_loss =0.3439852297306061\n",
            "Epoch = 91,.....trainning_loss=0.36099618673324585.....test_loss =0.3386552631855011\n",
            "Epoch = 92,.....trainning_loss=0.355874240398407.....test_loss =0.3333893418312073\n",
            "Epoch = 93,.....trainning_loss=0.35081279277801514.....test_loss =0.3281882405281067\n",
            "Epoch = 94,.....trainning_loss=0.3458113372325897.....test_loss =0.3230416774749756\n",
            "Epoch = 95,.....trainning_loss=0.3408728837966919.....test_loss =0.3179611265659332\n",
            "Epoch = 96,.....trainning_loss=0.33600279688835144.....test_loss =0.3129524886608124\n",
            "Epoch = 97,.....trainning_loss=0.33120375871658325.....test_loss =0.3080243468284607\n",
            "Epoch = 98,.....trainning_loss=0.3264774680137634.....test_loss =0.3031761348247528\n",
            "Epoch = 99,.....trainning_loss=0.32182490825653076.....test_loss =0.2984052896499634\n",
            "Epoch = 100,.....trainning_loss=0.3172488808631897.....test_loss =0.29372382164001465\n",
            "Epoch = 101,.....trainning_loss=0.3127502501010895.....test_loss =0.28913643956184387\n",
            "Epoch = 102,.....trainning_loss=0.3083289861679077.....test_loss =0.28464043140411377\n",
            "Epoch = 103,.....trainning_loss=0.3039947748184204.....test_loss =0.2802315950393677\n",
            "Epoch = 104,.....trainning_loss=0.29974469542503357.....test_loss =0.27590760588645935\n",
            "Epoch = 105,.....trainning_loss=0.2955819368362427.....test_loss =0.271676242351532\n",
            "Epoch = 106,.....trainning_loss=0.2915075719356537.....test_loss =0.2675429880619049\n",
            "Epoch = 107,.....trainning_loss=0.28751954436302185.....test_loss =0.2635088264942169\n",
            "Epoch = 108,.....trainning_loss=0.28361761569976807.....test_loss =0.25957098603248596\n",
            "Epoch = 109,.....trainning_loss=0.27980244159698486.....test_loss =0.2557276785373688\n",
            "Epoch = 110,.....trainning_loss=0.276073157787323.....test_loss =0.2519757151603699\n",
            "Epoch = 111,.....trainning_loss=0.2724265158176422.....test_loss =0.24831253290176392\n",
            "Epoch = 112,.....trainning_loss=0.26886287331581116.....test_loss =0.24473941326141357\n",
            "Epoch = 113,.....trainning_loss=0.2653816044330597.....test_loss =0.24125505983829498\n",
            "Epoch = 114,.....trainning_loss=0.26198074221611023.....test_loss =0.23785942792892456\n",
            "Epoch = 115,.....trainning_loss=0.2586614489555359.....test_loss =0.2345508486032486\n",
            "Epoch = 116,.....trainning_loss=0.2554197311401367.....test_loss =0.2313295304775238\n",
            "Epoch = 117,.....trainning_loss=0.2522542476654053.....test_loss =0.22819405794143677\n",
            "Epoch = 118,.....trainning_loss=0.24916277825832367.....test_loss =0.2251422107219696\n",
            "Epoch = 119,.....trainning_loss=0.2461465746164322.....test_loss =0.2221682369709015\n",
            "Epoch = 120,.....trainning_loss=0.2432057112455368.....test_loss =0.2192682921886444\n",
            "Epoch = 121,.....trainning_loss=0.2403341382741928.....test_loss =0.21644052863121033\n",
            "Epoch = 122,.....trainning_loss=0.2375289350748062.....test_loss =0.21368218958377838\n",
            "Epoch = 123,.....trainning_loss=0.23478786647319794.....test_loss =0.2109927237033844\n",
            "Epoch = 124,.....trainning_loss=0.23210884630680084.....test_loss =0.20836955308914185\n",
            "Epoch = 125,.....trainning_loss=0.22949020564556122.....test_loss =0.205810084939003\n",
            "Epoch = 126,.....trainning_loss=0.22692576050758362.....test_loss =0.20331323146820068\n",
            "Epoch = 127,.....trainning_loss=0.22441618144512177.....test_loss =0.20090201497077942\n",
            "Epoch = 128,.....trainning_loss=0.22196727991104126.....test_loss =0.19855228066444397\n",
            "Epoch = 129,.....trainning_loss=0.21958321332931519.....test_loss =0.19624781608581543\n",
            "Epoch = 130,.....trainning_loss=0.21724946796894073.....test_loss =0.19400571286678314\n",
            "Epoch = 131,.....trainning_loss=0.21496060490608215.....test_loss =0.19182118773460388\n",
            "Epoch = 132,.....trainning_loss=0.2127155363559723.....test_loss =0.18969008326530457\n",
            "Epoch = 133,.....trainning_loss=0.21052202582359314.....test_loss =0.18760338425636292\n",
            "Epoch = 134,.....trainning_loss=0.2083766609430313.....test_loss =0.1855582594871521\n",
            "Epoch = 135,.....trainning_loss=0.20627188682556152.....test_loss =0.18355898559093475\n",
            "Epoch = 136,.....trainning_loss=0.20420505106449127.....test_loss =0.18160590529441833\n",
            "Epoch = 137,.....trainning_loss=0.2021840512752533.....test_loss =0.17969311773777008\n",
            "Epoch = 138,.....trainning_loss=0.20019923150539398.....test_loss =0.17781981825828552\n",
            "Epoch = 139,.....trainning_loss=0.1982462853193283.....test_loss =0.1759757399559021\n",
            "Epoch = 140,.....trainning_loss=0.19632673263549805.....test_loss =0.17416788637638092\n",
            "Epoch = 141,.....trainning_loss=0.19443994760513306.....test_loss =0.17240211367607117\n",
            "Epoch = 142,.....trainning_loss=0.1925913542509079.....test_loss =0.17068123817443848\n",
            "Epoch = 143,.....trainning_loss=0.19077403843402863.....test_loss =0.16899070143699646\n",
            "Epoch = 144,.....trainning_loss=0.1889832615852356.....test_loss =0.1673399806022644\n",
            "Epoch = 145,.....trainning_loss=0.18722057342529297.....test_loss =0.16572165489196777\n",
            "Epoch = 146,.....trainning_loss=0.18548747897148132.....test_loss =0.16413578391075134\n",
            "Epoch = 147,.....trainning_loss=0.18378247320652008.....test_loss =0.162580206990242\n",
            "Epoch = 148,.....trainning_loss=0.18210557103157043.....test_loss =0.1610538214445114\n",
            "Epoch = 149,.....trainning_loss=0.1804576814174652.....test_loss =0.15955884754657745\n",
            "Epoch = 150,.....trainning_loss=0.17883537709712982.....test_loss =0.15809085965156555\n",
            "Epoch = 151,.....trainning_loss=0.1772371381521225.....test_loss =0.15665000677108765\n",
            "Epoch = 152,.....trainning_loss=0.17566204071044922.....test_loss =0.1552310287952423\n",
            "Epoch = 153,.....trainning_loss=0.17411157488822937.....test_loss =0.15383388102054596\n",
            "Epoch = 154,.....trainning_loss=0.17258431017398834.....test_loss =0.15245755016803741\n",
            "Epoch = 155,.....trainning_loss=0.1710795909166336.....test_loss =0.15110373497009277\n",
            "Epoch = 156,.....trainning_loss=0.1695965677499771.....test_loss =0.1497737318277359\n",
            "Epoch = 157,.....trainning_loss=0.1681346893310547.....test_loss =0.14846788346767426\n",
            "Epoch = 158,.....trainning_loss=0.16669440269470215.....test_loss =0.1471875160932541\n",
            "Epoch = 159,.....trainning_loss=0.16527466475963593.....test_loss =0.1459367722272873\n",
            "Epoch = 160,.....trainning_loss=0.16387435793876648.....test_loss =0.1447247862815857\n",
            "Epoch = 161,.....trainning_loss=0.1624973565340042.....test_loss =0.14352698624134064\n",
            "Epoch = 162,.....trainning_loss=0.16114048659801483.....test_loss =0.1423267275094986\n",
            "Epoch = 163,.....trainning_loss=0.15979762375354767.....test_loss =0.14114339649677277\n",
            "Epoch = 164,.....trainning_loss=0.15847282111644745.....test_loss =0.13996990025043488\n",
            "Epoch = 165,.....trainning_loss=0.15716342628002167.....test_loss =0.1388206034898758\n",
            "Epoch = 166,.....trainning_loss=0.15587161481380463.....test_loss =0.13770009577274323\n",
            "Epoch = 167,.....trainning_loss=0.15460504591464996.....test_loss =0.13660012185573578\n",
            "Epoch = 168,.....trainning_loss=0.15334905683994293.....test_loss =0.13551360368728638\n",
            "Epoch = 169,.....trainning_loss=0.15210561454296112.....test_loss =0.13444499671459198\n",
            "Epoch = 170,.....trainning_loss=0.15088202059268951.....test_loss =0.13339762389659882\n",
            "Epoch = 171,.....trainning_loss=0.14967383444309235.....test_loss =0.13236995041370392\n",
            "Epoch = 172,.....trainning_loss=0.14847911894321442.....test_loss =0.13136078417301178\n",
            "Epoch = 173,.....trainning_loss=0.14729759097099304.....test_loss =0.13036876916885376\n",
            "Epoch = 174,.....trainning_loss=0.1461290419101715.....test_loss =0.129391610622406\n",
            "Epoch = 175,.....trainning_loss=0.14498165249824524.....test_loss =0.12843753397464752\n",
            "Epoch = 176,.....trainning_loss=0.1438528597354889.....test_loss =0.12748895585536957\n",
            "Epoch = 177,.....trainning_loss=0.14273765683174133.....test_loss =0.12652836740016937\n",
            "Epoch = 178,.....trainning_loss=0.1416335105895996.....test_loss =0.12557469308376312\n",
            "Epoch = 179,.....trainning_loss=0.14054302871227264.....test_loss =0.12462987750768661\n",
            "Epoch = 180,.....trainning_loss=0.13946497440338135.....test_loss =0.12369455397129059\n",
            "Epoch = 181,.....trainning_loss=0.13839946687221527.....test_loss =0.1227751076221466\n",
            "Epoch = 182,.....trainning_loss=0.1373477578163147.....test_loss =0.12188957631587982\n",
            "Epoch = 183,.....trainning_loss=0.13630884885787964.....test_loss =0.12102117389440536\n",
            "Epoch = 184,.....trainning_loss=0.13527828454971313.....test_loss =0.1201687678694725\n",
            "Epoch = 185,.....trainning_loss=0.13425953686237335.....test_loss =0.11933065950870514\n",
            "Epoch = 186,.....trainning_loss=0.1332533061504364.....test_loss =0.11850481480360031\n",
            "Epoch = 187,.....trainning_loss=0.1322617530822754.....test_loss =0.11769460141658783\n",
            "Epoch = 188,.....trainning_loss=0.13128018379211426.....test_loss =0.11689367145299911\n",
            "Epoch = 189,.....trainning_loss=0.1303102970123291.....test_loss =0.11610498279333115\n",
            "Epoch = 190,.....trainning_loss=0.12935686111450195.....test_loss =0.11530622839927673\n",
            "Epoch = 191,.....trainning_loss=0.12841621041297913.....test_loss =0.11447911709547043\n",
            "Epoch = 192,.....trainning_loss=0.12748244404792786.....test_loss =0.11363226175308228\n",
            "Epoch = 193,.....trainning_loss=0.12656590342521667.....test_loss =0.11283338814973831\n",
            "Epoch = 194,.....trainning_loss=0.1256592571735382.....test_loss =0.11206147819757462\n",
            "Epoch = 195,.....trainning_loss=0.12475608289241791.....test_loss =0.11131605505943298\n",
            "Epoch = 196,.....trainning_loss=0.12386193871498108.....test_loss =0.11059382557868958\n",
            "Epoch = 197,.....trainning_loss=0.12297723442316055.....test_loss =0.10988704115152359\n",
            "Epoch = 198,.....trainning_loss=0.12210332602262497.....test_loss =0.10919295996427536\n",
            "Epoch = 199,.....trainning_loss=0.12124096602201462.....test_loss =0.1084914281964302\n",
            "Epoch = 200,.....trainning_loss=0.12038926035165787.....test_loss =0.10778950154781342\n",
            "Epoch = 201,.....trainning_loss=0.11954686045646667.....test_loss =0.10707705467939377\n",
            "Epoch = 202,.....trainning_loss=0.11870697140693665.....test_loss =0.10631876438856125\n",
            "Epoch = 203,.....trainning_loss=0.11786411702632904.....test_loss =0.1055276095867157\n",
            "Epoch = 204,.....trainning_loss=0.11702223867177963.....test_loss =0.10470117628574371\n",
            "Epoch = 205,.....trainning_loss=0.11617664992809296.....test_loss =0.10388415306806564\n",
            "Epoch = 206,.....trainning_loss=0.1153496652841568.....test_loss =0.10308205336332321\n",
            "Epoch = 207,.....trainning_loss=0.11451756209135056.....test_loss =0.10226154327392578\n",
            "Epoch = 208,.....trainning_loss=0.11369737237691879.....test_loss =0.10144057124853134\n",
            "Epoch = 209,.....trainning_loss=0.11287876963615417.....test_loss =0.10064522176980972\n",
            "Epoch = 210,.....trainning_loss=0.11207932978868484.....test_loss =0.09986851364374161\n",
            "Epoch = 211,.....trainning_loss=0.1112937182188034.....test_loss =0.09910634160041809\n",
            "Epoch = 212,.....trainning_loss=0.11052664369344711.....test_loss =0.09832530468702316\n",
            "Epoch = 213,.....trainning_loss=0.10976381599903107.....test_loss =0.09758848696947098\n",
            "Epoch = 214,.....trainning_loss=0.10901428759098053.....test_loss =0.09686728566884995\n",
            "Epoch = 215,.....trainning_loss=0.1082812249660492.....test_loss =0.09616974741220474\n",
            "Epoch = 216,.....trainning_loss=0.10755782574415207.....test_loss =0.0954989492893219\n",
            "Epoch = 217,.....trainning_loss=0.10684189200401306.....test_loss =0.09485610574483871\n",
            "Epoch = 218,.....trainning_loss=0.10614276677370071.....test_loss =0.09425891935825348\n",
            "Epoch = 219,.....trainning_loss=0.10545305162668228.....test_loss =0.09369195997714996\n",
            "Epoch = 220,.....trainning_loss=0.1047692745923996.....test_loss =0.09315335005521774\n",
            "Epoch = 221,.....trainning_loss=0.10409384965896606.....test_loss =0.09263887256383896\n",
            "Epoch = 222,.....trainning_loss=0.10342659056186676.....test_loss =0.09214165806770325\n",
            "Epoch = 223,.....trainning_loss=0.10276778787374496.....test_loss =0.09165841341018677\n",
            "Epoch = 224,.....trainning_loss=0.10212011635303497.....test_loss =0.0911957174539566\n",
            "Epoch = 225,.....trainning_loss=0.10148132592439651.....test_loss =0.09074939042329788\n",
            "Epoch = 226,.....trainning_loss=0.1008501797914505.....test_loss =0.09031375497579575\n",
            "Epoch = 227,.....trainning_loss=0.10022684931755066.....test_loss =0.08988416939973831\n",
            "Epoch = 228,.....trainning_loss=0.09961167722940445.....test_loss =0.0894508808851242\n",
            "Epoch = 229,.....trainning_loss=0.09900489449501038.....test_loss =0.08901569247245789\n",
            "Epoch = 230,.....trainning_loss=0.0984056144952774.....test_loss =0.08857375383377075\n",
            "Epoch = 231,.....trainning_loss=0.0978139340877533.....test_loss =0.08813516050577164\n",
            "Epoch = 232,.....trainning_loss=0.09723212569952011.....test_loss =0.08770546317100525\n",
            "Epoch = 233,.....trainning_loss=0.09665780514478683.....test_loss =0.0872875526547432\n",
            "Epoch = 234,.....trainning_loss=0.09609072655439377.....test_loss =0.08688443154096603\n",
            "Epoch = 235,.....trainning_loss=0.09553059935569763.....test_loss =0.08649636059999466\n",
            "Epoch = 236,.....trainning_loss=0.09497734904289246.....test_loss =0.08611990511417389\n",
            "Epoch = 237,.....trainning_loss=0.09443088620901108.....test_loss =0.08574990928173065\n",
            "Epoch = 238,.....trainning_loss=0.0938912034034729.....test_loss =0.08538135886192322\n",
            "Epoch = 239,.....trainning_loss=0.09335815906524658.....test_loss =0.08501371741294861\n",
            "Epoch = 240,.....trainning_loss=0.09283146262168884.....test_loss =0.08464480191469193\n",
            "Epoch = 241,.....trainning_loss=0.09231191128492355.....test_loss =0.08425383269786835\n",
            "Epoch = 242,.....trainning_loss=0.09179927408695221.....test_loss =0.08384855091571808\n",
            "Epoch = 243,.....trainning_loss=0.09129365533590317.....test_loss =0.08340338617563248\n",
            "Epoch = 244,.....trainning_loss=0.09079238772392273.....test_loss =0.08297868072986603\n",
            "Epoch = 245,.....trainning_loss=0.09029804915189743.....test_loss =0.08258827775716782\n",
            "Epoch = 246,.....trainning_loss=0.08980873972177505.....test_loss =0.08223600685596466\n",
            "Epoch = 247,.....trainning_loss=0.08932541310787201.....test_loss =0.08190163224935532\n",
            "Epoch = 248,.....trainning_loss=0.08884864300489426.....test_loss =0.08157182484865189\n",
            "Epoch = 249,.....trainning_loss=0.08837870508432388.....test_loss =0.081243135035038\n",
            "Epoch = 250,.....trainning_loss=0.08791423588991165.....test_loss =0.08090732991695404\n",
            "Epoch = 251,.....trainning_loss=0.08745453506708145.....test_loss =0.08056295663118362\n",
            "Epoch = 252,.....trainning_loss=0.08699975162744522.....test_loss =0.08018111437559128\n",
            "Epoch = 253,.....trainning_loss=0.08654991537332535.....test_loss =0.07977940887212753\n",
            "Epoch = 254,.....trainning_loss=0.08610685914754868.....test_loss =0.07942000776529312\n",
            "Epoch = 255,.....trainning_loss=0.08566993474960327.....test_loss =0.07910750061273575\n",
            "Epoch = 256,.....trainning_loss=0.08523593097925186.....test_loss =0.0788358747959137\n",
            "Epoch = 257,.....trainning_loss=0.08481022715568542.....test_loss =0.07852812856435776\n",
            "Epoch = 258,.....trainning_loss=0.08438768982887268.....test_loss =0.0781840980052948\n",
            "Epoch = 259,.....trainning_loss=0.08396829664707184.....test_loss =0.07783731818199158\n",
            "Epoch = 260,.....trainning_loss=0.08355489373207092.....test_loss =0.0775122418999672\n",
            "Epoch = 261,.....trainning_loss=0.08314645290374756.....test_loss =0.07720909267663956\n",
            "Epoch = 262,.....trainning_loss=0.08274193853139877.....test_loss =0.07692418247461319\n",
            "Epoch = 263,.....trainning_loss=0.08234596997499466.....test_loss =0.07661574333906174\n",
            "Epoch = 264,.....trainning_loss=0.08195337653160095.....test_loss =0.07628638297319412\n",
            "Epoch = 265,.....trainning_loss=0.08156351745128632.....test_loss =0.07596302777528763\n",
            "Epoch = 266,.....trainning_loss=0.08117762207984924.....test_loss =0.07564792037010193\n",
            "Epoch = 267,.....trainning_loss=0.08079733699560165.....test_loss =0.07533254474401474\n",
            "Epoch = 268,.....trainning_loss=0.0804218053817749.....test_loss =0.07504482567310333\n",
            "Epoch = 269,.....trainning_loss=0.08005055785179138.....test_loss =0.07478055357933044\n",
            "Epoch = 270,.....trainning_loss=0.07968262583017349.....test_loss =0.07451411336660385\n",
            "Epoch = 271,.....trainning_loss=0.07931949198246002.....test_loss =0.07424428313970566\n",
            "Epoch = 272,.....trainning_loss=0.07896070927381516.....test_loss =0.07394932210445404\n",
            "Epoch = 273,.....trainning_loss=0.078607477247715.....test_loss =0.07365040481090546\n",
            "Epoch = 274,.....trainning_loss=0.07825850695371628.....test_loss =0.07338238507509232\n",
            "Epoch = 275,.....trainning_loss=0.07791285961866379.....test_loss =0.07314437627792358\n",
            "Epoch = 276,.....trainning_loss=0.07757165282964706.....test_loss =0.07289482653141022\n",
            "Epoch = 277,.....trainning_loss=0.07723232358694077.....test_loss =0.07263194769620895\n",
            "Epoch = 278,.....trainning_loss=0.07689781486988068.....test_loss =0.07237958163022995\n",
            "Epoch = 279,.....trainning_loss=0.076568603515625.....test_loss =0.07211174070835114\n",
            "Epoch = 280,.....trainning_loss=0.07624304294586182.....test_loss =0.07185975462198257\n",
            "Epoch = 281,.....trainning_loss=0.07592081278562546.....test_loss =0.0716017484664917\n",
            "Epoch = 282,.....trainning_loss=0.07560174912214279.....test_loss =0.07134520262479782\n",
            "Epoch = 283,.....trainning_loss=0.07528677582740784.....test_loss =0.0711069256067276\n",
            "Epoch = 284,.....trainning_loss=0.07497608661651611.....test_loss =0.07091081887483597\n",
            "Epoch = 285,.....trainning_loss=0.0746678039431572.....test_loss =0.07071953266859055\n",
            "Epoch = 286,.....trainning_loss=0.07436298578977585.....test_loss =0.07051542401313782\n",
            "Epoch = 287,.....trainning_loss=0.07406285405158997.....test_loss =0.07029172033071518\n",
            "Epoch = 288,.....trainning_loss=0.07376517355442047.....test_loss =0.07005297392606735\n",
            "Epoch = 289,.....trainning_loss=0.07347080111503601.....test_loss =0.06983444094657898\n",
            "Epoch = 290,.....trainning_loss=0.07318045943975449.....test_loss =0.06961344182491302\n",
            "Epoch = 291,.....trainning_loss=0.07289356738328934.....test_loss =0.06939707696437836\n",
            "Epoch = 292,.....trainning_loss=0.07260847836732864.....test_loss =0.06918973475694656\n",
            "Epoch = 293,.....trainning_loss=0.07232605665922165.....test_loss =0.06896692514419556\n",
            "Epoch = 294,.....trainning_loss=0.07205014675855637.....test_loss =0.0687592476606369\n",
            "Epoch = 295,.....trainning_loss=0.07177682965993881.....test_loss =0.0685732513666153\n",
            "Epoch = 296,.....trainning_loss=0.07150529325008392.....test_loss =0.06840415298938751\n",
            "Epoch = 297,.....trainning_loss=0.07123561948537827.....test_loss =0.06824925541877747\n",
            "Epoch = 298,.....trainning_loss=0.07096997648477554.....test_loss =0.06810126453638077\n",
            "Epoch = 299,.....trainning_loss=0.07070693373680115.....test_loss =0.06794394552707672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat11 = mod(X_train_new)\n",
        "y_hat11.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99YOf-L6IMGc",
        "outputId": "1a11c656-2f72-471e-dc4c-337aa4fb7aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([455, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ox2ldLjDIO8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1NsP_dNILkQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION THREE-3**"
      ],
      "metadata": {
        "id": "b1-gSd2KLk3x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9dl1jDEfLq1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QkwS61JeLq4i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}